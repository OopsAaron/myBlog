---
title: 2020-08-29-temp
mathjax: true
date: 2020-08-29 21:01:01
tags:
top:
categories:
description: 杂
---

# 哈希革新Transformer：这篇ICLR高分论文让一块GPU处理64K长度序列

Transformer 是近期 NLP 领域里最热门的模型之一，但因为算力消耗过大，对于个人研究者来说一直不太友好。近日一篇入选 ICLR 2020 的`研究提出了「Reformer」，把跑 Transformer 模型的硬件要求压缩到了只需一块 GPU，同时效果不变。`

大型的 Transformer 往往可以在许多任务上实现 sota，但训练这些模型的成本很高，尤其是在序列较长的时候。在 ICLR 的入选论文中，我们发现了一篇由谷歌和伯克利研究者发表的优质论文。文章介绍了两种提高 Transformer 效率的技术，`**最终的 Reformer 模型和 Transformer 模型在性能上表现相似，并且在长序列中拥有更高的存储效率和更快的速度**`。论文最终获得了「8，8，6」的高分。

![img](E:\myBlog\source\_posts\image.png)

在最开始，文章提出了`将点乘注意力（dot-product attention）替换为一个使用局部敏感哈希（locality-sensitive hashing）的点乘注意力，将复杂度从 O(L2 ) 变为 O(L log L)，此处 L 指序列的长度`。



此外，研究者使用`可逆残差（reversible residual layers）代替标准残差（standard residuals），这使得存储在训练过程中仅激活一次，而不是 n 次（此处 n 指层数）`。最终的 Reformer 模型和 Transformer 模型在性能上表现相同，同时在长序列中拥有更高的存储效率和更快的速度。



这篇论文在评审过程中收获了「一致通过」，并被认为将产生重大影响，也经过了几位外部评审的详细审查，最终获得了「8，8，6」的高分。



- 论文地址：https://openreview.net/forum?id=rkgNKkHtvB
- 代码：https://github.com/google/trax/blob/master/trax/models/research/reformer.py



**引言**



Transformer 架构被广泛用于自然语言处理中，并且在许多任务中实现了 sota。为了获得这些结果，研究者不得不开始训练更大的 Transformer 模型。在最大的配置中，参数数量已经超过了 0.5B/层，层数多达 64。



诸如此类的大型 Transformer 模型频频出现，到底是客观上必须要求如此多的资源，还是仅仅是因为处理效率不够高？



可以参考下面这些数据：



0.5B 的参数占据了 2GB 的内存，嵌入大小为 1024、批处理大小为 8 的 64K token 的激活要用 64K×1K×8 = 0.5B 浮点数，需要另外 2GB 的内存。



如果说每层的内存占用只有这么一些的话，部署 Transformer 会比实际中更容易，但是事情并不是这样的。以上的估计只包括了每层的内存占用情况和输入的激活损失，并没有考虑 Transformer 上的内存占用问题：



- 由于激活需要被存储并用于反向传播，有着 N 层的模型的大小比单层大了 N 倍；
- 由于中间的全连接层的深度 d_ff 通常远大于注意力激活层的深度 d_model，因此需要占用很大的内存；
- 在长度为 L 的序列上的 attention 的计算和时间复杂度是 O(L2)，所以即使是一个有 64K 字符的序列就会耗尽 GPU 的内存。



研究者提出了一种 Reformer 模型来解决刚才说的那些问题：



- 可逆层（Reversible layer），这个东西最早是 Gomez 等人引入的，在整个模型中启用单个副本，所以 N factor 就消失了；
- 在前馈层（feed-forward layer）分开激活和分块处理，消除 d_ff factor，节省前馈层的内存；
- 基于局部敏感哈希（locality-sensitive hashing，LSH）的近似注意力计算，让注意力层的 O(L2) 因子替代 O(L) 因子，实现在长序列上的操作。



### 局部敏感哈希

**局部敏感哈希的基本思想类似于一种空间域转换思想，LSH算法基于一个假设，如果两个文本在原有的数据空间是相似的，那么分别经过哈希函数转换以后的****它们也具有很高的相似度；相反，如果它们本身是不相似的，那么经过转换后它们应仍不具有相似性。**

局部敏感哈希这样的哈希函数可以具有上述的功能，可以保持数据转化前后的相似性

局部敏感哈希的最大特点就在于保持数据的相似性

==========

理解语言、音乐或视频之类`序列数据 (Sequential Data) 的任务充满挑战，尤其是当这些数据依赖于时间跨度较广的上下文时。`

 

`例如，在一个视频中，如果某个人或某个物体在消失很长一段时间后才重新出现，许多模型会忘记它的样子。`而在语言领域，长短期记忆 (LSTM) 神经网络会考虑足够多的上下文来执行 逐句翻译 。在这种情况下，上下文窗口 (Context Window) 的范围（即翻译中考虑的数据范围）会涵盖数十到上百个单词。

 

最新的 Transformer [模型](https://flashgene.com/archives/tag/模型) 不仅`提高了逐句翻译的性能`，而且可以用于通过多文档摘要生成完整的 Wikipedia 文章 。`由于 Transformer 使用的上下文窗口扩展到了数千个单词，使上述情景得以实现。凭借宽泛的上下文窗口，Transformer 的应用范围从文本扩展到了包括像素、音符在内的其他场景，可以用于生成 音乐 和 图像 。`

 

但是，Transformer 的`上下文窗口有限制范围。`

 

Transformer 的强大来源于 *注意力* (Attention) 机制 ，通过这一机制，Transformer 将上下文窗口内所有可能的单词对纳入考虑，以理解它们之间的联系。因此，如果文本包含 10 万个单词，Transformer 将需要评估 100 亿单词对（10 万 x 10 万），这显然不切实际。 

另一个实践问题是 `如何保存每个模型层的输出` 。对于使用大型上下文窗口的应用来说，存储多个模型层输出的内存需求会迅速变得过大（从几层模型层的数 G 级别到 [数千层](https://mp.weixin.qq.com/s?__biz=MzAwODY4OTk2Mg==&mid=2652051496&idx=1&sn=d15b291b45dc090187bcb0714f76a65b&chksm=808cb86db7fb317b0e26802cb0929e9e8462357f7c0d786471e15131d735c94adb881bfbe47d&scene=21#wechat_redirect) 模型层的数 T 级别）。`这意味着，实际使用大量层的 Transformer 模型只能用于生成几小段落的文本或一小段的音乐。`

 

今天，我们推出了 [Reformer](https://flashgene.com/archives/tag/reformer) ，一个设计为处理多达 100 万单词的上下文窗口的 Transformer 模型，所有工作在单个加速器上进行且仅使用 16 GB 内存。`此模型将综合运用两种关键技术来解决 Transformer 在长上下文窗口的注意力和内存分配问题的应用限制`。[Reformer](https://flashgene.com/archives/tag/reformer) `使用局部敏感哈希 (Locality-Sensitive-Hashing, LSH) 来降低长序列的处理复杂度和 可逆残差层 ，从而更有效地使用可用内存。`





#### 注意力问题

 

将 Transformer 模型应用于宽范围的文本序列时，第一个挑战是如何处理注意力层。

 

局部敏感哈希的挑战在于通过计算向量匹配哈希函数，而不是搜索所有可能的向量。例如，在翻译任务中，来自网络首层的每个向量表示一个单词（甚至是后续层中更大量的上下文），而不同语言中相同单词的向量可获得相同的哈希值。

 

在下图中，不同的颜色表示不同的哈希值，相似的词则具有相同的颜色。分配哈希值后，序列重新排列，将具有相同哈希值的元素放在一起，再分为多个片段（或多个区块）以实现并行处理。然后在这些短得多的区块（及其相近邻块以覆盖溢出）内应用注意力，从而大大降低计算负载。



![image-20200829223953263](E:\myBlog\source\_posts\image-20200829223953263.png)

局部敏感哈希：Reformer 接收 Key 的序列，其中每个键值代表首层中每个单词（在图像的情况下为像素）和后续层中大量的上下文向量。将局部敏感哈希应用到序列之后，依照键值的哈希值对其排序并分块。注意力仅针对于单个区块及其直接邻块



#### 内存问题

 

虽然局部敏感哈希解决了注意力问题，但仍然存在内存问题。

 

`一个单层的网络一般需要多达几 GB 的内存以及单个 GPU，因此，单层能够执行长序列的单模型。但是，当使用梯度下降训练多层模型时，由于需要保存每一层的激活（函数），以用于执行逆推。一个传统的 Transformer 模型具有十几个或更多的层，通过缓存这些层的值，内存将会很快用完。`

 

第二种 Reformer 中实现的新方法是在反向传播期 (Back-Propagation) 间，按需重新计算每个层的输入，而不是将其保存在内存中。这是使用 可逆层 来实现的，其中来自网络最后一层的激活 (Activation) 用于还原来自任何中间层的激活 (Activation)，这相当于反向运行网络。

 

在典型的残差网络中，通过网络传递的输入将会向堆栈中的每一层不断添加至向量。相反，可逆层中每个层有两组激活。一组遵循刚才描述的标准过程，从一层逐步更新到下一层，但是另一组仅捕获第一层的变更。因此，若要反向运行网络，只需简单地减去每一层应用的激活。

![image-20200829225435706](E:\myBlog\source\_posts\image-20200829225435706.png)

可逆层：（图 a）在标准残差网络中，每一层的激活用于将输入更新到下一层。（图 b）在可逆网络中，将会维持两组激活，只有其中一组逐层更新。（图 c）这种方法可使网络反向运行，以还原所有中间值



#### Reformer 的应用

 

`这两种新方法能让 Reformer 具有更高的效率，可以在仅有 16GB 内存的单个加速器上处理长达一百万个单词的文本序列。正因为 Reformer 的高效率，可以被直接应用于上下文窗口远大于任何最新 (SOTA) 文本域数据集的数据。借助 Reformer 能处理如此大规模数据集的能力，可促进社区创建大规模数据集的速度。`



在不缺乏大规模上下文数据的领域中，我们选择图像生成作为我们实验 Reformer 的对象。在这份 Colab 笔记 [1] 中，我们提供了一些示例，说明如何使用 Reformer 将部分图像变得“完整”。从下图上行所示的图像片段开始，Reformer 可以逐像素生成全画幅图像（下行）。



========

## The Reformer

"Reformer: The Efficient Transformer"的作者采用了`一种完全不同的方法来处理序列长度问题`。首先，他们观察到学习不同的keys和queries的投影并不是严格必要的。`他们丢弃了query投影，并将注意力权重替换为key的函数。`



现在，注意力块不再包含queries的单独投影，我们只有key和value对。然而，`计算key的协同矩阵(通过将每个key与其他key进行比较)仍然是非常昂贵的。`

不幸的是我们可能并没有利用好所有的这些计算。softmax的输出通常由几个关键元素控制 — 其余的往往在噪声中消失。`我们在计算softmax的时候，并不一定需要那些注意力权重很小的token。`



在编写传统软件时，我们总是会遇到这个问题。如果我们`想找到与给定key对应的value，我们通常不会遍历所有key的列表并检查每个key是否匹配。`相反，我们使用散列映射数据结构来执行O(1)的查找，而不是O(n)比较。

方便的是，`向量空间的哈希映射确实存在类似的情况，它被称为“局部敏感哈希”(LSH)。正是基于这种方法，Reformer的论文的作者们希望产生一个transformer的替代方案，以避免使用点积注意力的平方复杂性。`



## 局部敏感哈希 (LSH)

局部敏感哈希是一组将高维向量映射到一组离散值(桶/集群)的方法。它最常用来作为近似最近邻搜索的一种方法，用于近似的重复检测或视觉搜索等应用。

局部敏感哈希方法尝试将高维空间中相近的向量以高概率分配到相同的哈希。`有效的哈希函数有很多种，最简单的可能是随机投影。`



换句话说，我们选择一个随机的向量集合，观察输入向量在每个向量上的投影是正的还是负的，然后使用这个二值向量来表示给定向量的预期存储区。下图说明了LSH投影矩阵“u”中单个向量的处理过程。绿色的正号表示与向量u点积为正的点，而红色的负号表示与向量u点积为负的点。

![640.png](E:\myBlog\source\_posts\bVAfX)

## LSH注意力

Reformer的论文选择了局部敏感哈希的angular变体。`它们首先约束每个输入向量的L2范数(即将向量投影到一个单位球面上)，然后应用一系列的旋转，最后找到每个旋转向量所属的切片。`

![640.jpg](E:\myBlog\source\_posts\bVAfZ)

该图演示了一个用`4个桶进行3轮哈希的设置`。下面的图中的向量映射到了同一个bucket，因为它们的输入很接近，而上一张图中的向量映射到第一个和最后一个bucket。

`在为每个token计算一个桶之后，将根据它们的桶对这些token进行排序，并将标准的点积注意力应用到桶中的token的块上。`

![640-1.png](E:\myBlog\source\_posts\bVAf0)

有了足够多的桶，这就大大减少了所有的给定的token需要处理的token的数量 —— 在实验中，Reformer的论文运行的模型被配置为使用128块大小的块。因此，LSH操作将昂贵的key协同矩阵乘法的上下文大小限制为更易于管理的值。

`我们现在的时间复杂度为O (L*log(L)) ，而不是时间复杂度成正比O (L²)， 这允许我们把注意力操作扩展到更长的序列的时候不会由于运行时间而受到影响。`

`因为这个分桶过程是随机的，所以Reformer有选择地多次运行这个过程，以减少两个在输入空间很近的向量被随机地放在不同的桶中的可能性。当所有的事情都做了之后，你就有了一个完全替代标准的多头注意力的方法`，它可以与计算完整的注意力矩阵相媲美。

## 内存复杂度

不幸的是，实现更好的时间复杂度只是问题的一半。`如果我们将新的LSH注意力块替换为标准的多头注意力，并尝试输入新长度的信息，我们将很快认识到系统中的下一个瓶颈 — 内存复杂性。`

即使我们已经非常小心地最小化了注意力操作的计算复杂度，`我们仍然必须将所有的key和value存储在内存中，更糟糕的是，在训练期间，我们需要缓存激活以计算参数更新。`

`Reformer论文使用了序列长度为64k的enwiki8语言建模数据集来做实验，隐藏单元的大小为1024，层数为12层，这意味着存储key和value需要2 * 64000 * 1024 * 12 = ~ 1.5B个浮点数，大约是6GB的内存。使用这种内存使用方式，我们将无法在训练期间使用大的批处理大小，从而影响我们的运行时间。`

一个选择是实现gradient checkpoint来帮助限制我们的内存使用。允许我们减少内存使用，只存储从正向传递中的关键的激活，剩余的在反向传递中重新计算。因此，我们可以选择只在key和value投影之前存储隐藏状态，而不是存储key和value，然后第二次重新投影隐藏状态来计算梯度。

不幸的是，这使我们的后向传递的成本增加了一倍，因此我们能够支持更大的批处理大小所获得的好处将通过重新计算得到部分缓解。更重要的是，即使我们选择只存储输入的一小部分，存储单个层的激活需要250MB的空间，这意味着我们很难在12GB的GPU上支持超过12个样本的批处理大小。



## Re**vN**ets

幸运的是，我们还有其他方法来减少内存使用。RevNet。

RevNets有个非常聪明的计算技巧，`通过以一种特定的方式构造每一层，使内存使用与网络深度保持一致。每一层分为两个部分，X₁和X₂`，前向计算



可视化一下，看起来就是这样：

![640-2.png](E:\myBlog\source\_posts\bVAf1)

来自RevNet论文的图，图(a)为RevNet的前向，图(b)为相应的反向。

由于该层的特定结构，我们可以编写一个自定义函数参数更新，这意味着我们不需要缓存任何激活来计算我们的后向传播。类似于使用梯度检查点，我们仍然需要做一些冗余计算。`然而，由于每一层的输入（x）都可以很容易地从它的输出（y）中构造出来，我们的内存使用不再随网络中层数的增加而增加。`

在实践中，`Reformer定义f(x)是LSH注意力块，g (x)是标准的前向块，来自transformer结构。`

![640-3.png](E:\myBlog\source\_posts\bVAf2)

`有了RevNet架构，我们只需要在内存中存储单层的激活，就可以在训练期间使用更大的批处理大小！现在我们不再受训练期间激活的内存占用的限制，我们可以利用LSH注意力块改进时间复杂度。`



重要的是，语言模型的loss不会因为可逆层结构而降低。

![640-4.png](E:\myBlog\source\_posts\bVAf3)

这些变化实现起来并不容易 —— 很明显Nikita Kitaev, Łukasz Kaiser和Anselm Levskaya付出巨大的努力在平衡时间和内存。

总的来说，这些变化使得序列长度的扩展成为可能。虽然结果是初步的，但在enwiki8上的实验表明，在语言建模任务上，Reformer可以与它的重量级前辈竞争。

![640-1.jpg](E:\myBlog\source\_posts\bVAf4)



实验结果

下图是不同的方法在这两个数据集上的表现，可以看到，无论是共享QK还是可逆Transformer，都不会影响效果。

![img](https://upload-images.jianshu.io/upload_images/17768144-79d4bfe33746f056.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)



下图是`不同哈希桶数的LSH注意力的表现`。显然，`数量越多，效果越好，这是因为关注就越精确，同时模型代价就越高。`

![img](https:////upload-images.jianshu.io/upload_images/17768144-cc4f71f61863a84b.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

最后是`Reformer的层数对于性能的影响`。下图（左）是Big Reformer随层变化的不同效果，20层依然无压力。而下图（右）是普通注意力和LSH注意力在不同序列长度的速度比较，当序列很长的时候，LSH具有显著的优势。

![img](https:////upload-images.jianshu.io/upload_images/17768144-2dd73f9b6168d019.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)



## 总结

------

位置敏感哈希的注意力和可逆层构成了Reformer的蓝图，非常高兴可以看到基于transformer的结构选择去优化和处理长序列的问题，而不是简单的扩展之前的工作。



===

Transformer中的注意力计算需要让矩阵Q和K的转置相乘。我们假定它们的形状都是[ batch_size, length, dimension ]，那么如果序列长度有64K，就有得到一个64K*64K的矩阵，显然是不现实的。

对于局部敏感哈希注意力而言，需要 Q=K，以及 V，它们的 shape 都是 [batch size,length,d_model]，而重点关注的是 QK^T，有着 [batch size,length,length] 的 shape。进一步来说，对于每个 q_i，实际需要关注的是它们在 key 的接近值。例如，如果 K 是 64K，对于每个 q_i，只需要考虑一小部分，如 32 个到 64 个最接近的 keys。

这样一来就需要找到最近邻的值，这就需要局部敏感哈希（LSH）了，它能够快速在高维空间中找到最近邻。一个局部敏感哈希算法可以将每个向量 x 转换为 hash h(x)，和这个 x 靠近的哈希更有可能有着相同的哈希值，而距离远的则不会。在这里，研究者希望最近的向量最可能得到相同的哈希值，或者 hash-bucket 大小相似的更有可能相同。



虽然 LSH 提升了时间效率，但仍然存在一个内存的问题。当训练一个具有梯度下降的多层模型时，需要保存每一层的激活值，以便在向后传递中使用。一个典型的 Transformer 模型有 12 个或更多的层，因此，如果用来缓存来自每个层的值，那么内存很快就会用完。

在 Reformer 中实现的第二个新方法是在反向传播期间按需重新计算每个层的输入，而不是将其存储在内存中。这是通过使用**可逆层**来实现的，其中来自网络的最后一层的激活被用来恢复来自任何中间层的激活，这相当于反向运行网络。在一个典型的残差网络中，栈中的每一层都不断地增加通过网络的向量。相反，可逆层对每个层有两组激活。一个遵循刚才描述的标准过程，并从一个层逐步更新到下一个层，但是另一个只捕获对第一个层的更改。因此，要反向运行网络，只需减去应用于每个层的激活。![img](E:\myBlog\source\_posts\17768144-92afd80bc919cfd1.png)



![img](E:\myBlog\source\_posts\17768144-aebc2f755db51a88.png)

