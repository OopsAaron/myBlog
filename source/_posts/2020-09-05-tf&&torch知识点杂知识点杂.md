---
title: 2020-09-05-tf&&torch知识点杂
mathjax: true
date: 2020-09-05 19:56:22
tags:
top: 80
categories:
description: 记录杂乱的知识点，持续更新
---



## 🚀others

### python的einops  rearrange()函数

例子：

假设我有一个3-D数组：

```
[[[0,1,2],
  [0,1,2],
  [0,1,2]],

 [[3,4,5],
  [3,4,5],
  [3,4,5]]]
```

我想按列重新排列：

```
[[0,1,2,3,4,5],
 [0,1,2,3,4,5],
 [0,1,2,3,4,5]]
```



使用einops：

```py
einops.rearrange(a, 'x y z -> y (x z) ')
```

并且我建议根据上下文（例如时间，高度等）为轴指定有意义的名称（而不是xyz）。 这将使您易于理解代码的作用

```
In : einops.rearrange(a, 'x y z -> y (x z) ')
Out:
array([[0, 1, 2, 3, 4, 5],
       [0, 1, 2, 3, 4, 5],
       [0, 1, 2, 3, 4, 5]])
```



### Epoch、Iteration、Batch_size

> [https://blog.csdn.net/program_developer/article/details/78597738](https://blog.csdn.net/program_developer/article/details/78597738)



## 🚀tensorflow

### tf.tile()用法

> [https://blog.csdn.net/tsyccnh/article/details/82459859](https://blog.csdn.net/tsyccnh/article/details/82459859)



### Dataset API 和 Iterator

> Dataset API 和 Iterator
>
> [https://blog.csdn.net/briblue/article/details/80962728](https://blog.csdn.net/briblue/article/details/80962728)
>
> TensorFlow中的Dataset API
>
> [https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79342369](https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79342369)
>
> TensorFlow data模块详解
>
> [https://www.weaf.top/posts/cd5ba0c4/](https://www.weaf.top/posts/cd5ba0c4/)
>
> 使用Tensorflow的DataSet和Iterator读取数据
>
> [https://www.jianshu.com/p/bcff8a99b15b](https://www.jianshu.com/p/bcff8a99b15b)
>
> tensorflow数据读取机制（附代码）
>
> [https://zhuanlan.zhihu.com/p/27238630](https://zhuanlan.zhihu.com/p/27238630)
>
> Dataset API入门教程
>
> [https://zhuanlan.zhihu.com/p/30751039](https://zhuanlan.zhihu.com/p/30751039)
>
> Dataset.from_generator
>
> [https://blog.csdn.net/foreseerwang/article/details/80572182](https://blog.csdn.net/foreseerwang/article/details/80572182)



看个简单的示例：

```
#创建一个Dataset对象
dataset = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8,9])

#创建一个迭代器
iterator = dataset.make_one_shot_iterator()

#get_next()函数可以帮助我们从迭代器中获取元素
element = iterator.get_next()

#遍历迭代器，获取所有元素
with tf.Session() as sess:
   for i in range(9):
       print(sess.run(element))
```

以上打印结果为：1 2 3 4 5 6 7 8 9

from_generator

创建Dataset由其生成元素的元素generator。

函数形式：from_generator(generator,output_types,output_shapes=None,args=None)

参数generator:一个可调用对象，它返回支持该iter()协议的对象 。如果args未指定，generator则不得参数; 否则它必须采取与有值一样多的参数args。
参数output_types：tf.DType对应于由元素生成的元素的每个组件的对象的嵌套结构generator。
参数output_shapes:tf.TensorShape 对应于由元素生成的元素的每个组件的对象 的嵌套结构generator
参数args:tf.Tensor将被计算并将generator作为NumPy数组参数传递的对象元组。

具体例子

```
#定义一个生成器
def data_generator():
    dataset = np.array(range(9))
    for i in dataset:
        yield i

#接收生成器，并生产dataset数据结构
dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32))

iterator = concat_dataset.make_one_shot_iterator()

element = iterator.get_next()

with tf.Session() as sess:
   for i in range(3):
       print(sess.run(element))
```

以上代码运行结果：0 1 2



### tf strip() 和  split() 

> [https://blog.csdn.net/hjxu2016/article/details/78676859](https://blog.csdn.net/hjxu2016/article/details/78676859)

### Summary用法 -tensorborad可视化

> [https://www.cnblogs.com/lyc-seu/p/8647792.html](https://www.cnblogs.com/lyc-seu/p/8647792.html)

### math.ceil() 

> [https://www.runoob.com/python/func-number-ceil.html](https://www.runoob.com/python/func-number-ceil.html)

### .format() 格式化函数

> [https://www.runoob.com/python/att-string-format.html](https://www.runoob.com/python/att-string-format.html)

### tf.shape(A) 和 A.get_shape().as_list() 和 tf.split()

> [https://www.itread01.com/content/1544436557.html](https://www.itread01.com/content/1544436557.html)
>
> [https://blog.csdn.net/xc_zhou/article/details/85632109](https://blog.csdn.net/xc_zhou/article/details/85632109)

- tf.shape(A) # 獲取張量A（陣列，list, tensor張量）的大小，返回的是一個list
- x.get_shape()，只有**tensor**才可以使用這種方法，返回的是一個元組
- tf.split(dimension, num_split, input)：dimension的意思就是輸入張量的哪一個維度，如果是0就表示對第0維度進行切割。num_split就是切割的數量，如果是2就表示輸入張量被切成2份，每一份是一個列表。

### tf.range()

```python
w=tf.range(3)
print (sess.run(w))#输出[0 1 2]
```

### os.path（）



| 方法                                | 说明                                                         |
| :---------------------------------- | :----------------------------------------------------------- |
| os.path.abspath(path)               | 返回绝对路径                                                 |
| os.path.basename(path)              | 返回文件名                                                   |
| os.path.join(path1[, path2[, ...]]) | 把目录和文件名合成一个路径                                   |
| os.path.dirname(path)               | 返回文件路径                                                 |
| os.path.exists(path)                | 如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。 |
| os.path.split(path)                 | 把路径分割成 dirname 和 basename，返回一个元组               |



> [https://www.runoob.com/python/python-os-path.html](https://www.runoob.com/python/python-os-path.html)

### embedding_lookup()

tf.nn.embedding_lookup()就是根据input_ids中的id，寻找embeddings中的第id行。比如input_ids=[1,3,5]，则找出embeddings中第1，3，5行，组成一个tensor返回。

> [https://www.jianshu.com/p/7bb87873f89e](https://www.jianshu.com/p/7bb87873f89e)
>
> [https://www.zhihu.com/question/52250059](https://www.zhihu.com/question/52250059)



### 模型保存和加载

Saver的作用是将我们训练好的模型的参数保存下来，以便下一次继续用于训练或测试；Restore的用法是将训练好的参数提取出来。 

1.Saver类训练完后，是以**checkpoints文件形式**保存。提取的时候也是从checkpoints文件中恢复变量。 Checkpoints文件是一个二进制文件，它把变量名映射到对应的tensor值 。 

2.通过for循环，Saver类可以自动的生成checkpoint文件。这样我们就可以**保存多个训练结果**。例如，我们可以保存每一步训练的结果。但是为了避免填满整个磁盘，**Saver可以自动的管理Checkpoints文件**。例如，我们可以指定保存最近的N个Checkpoints文件。



### Tensorflow模型保存和读取tf.train.Saver

目标：训练网络后想保存训练好的模型，以及在程序中读取以保存的训练好的模型。

首先，保存和恢复都需要实例化一个 tf.train.Saver。

>  saver = tf.train.Saver() 

然后，在训练循环中，定期调用 saver.save() 方法，向文件夹中写入包含了当前模型中所有可训练变量的 checkpoint 文件。

>  saver.save(sess, save_path, global_step=step) 

之后，就可以使用 saver.restore() 方法，重载模型的参数，继续训练或用于测试数据。

>  saver.restore(sess, save_path) 

模型的恢复用的是restore()函数，它需要两个参数restore(sess, save_path)，save_path指的是保存的模型路径。我们可以使用`tf.train.latest_checkpoint（）`来自动获取最后一次保存的模型。如：

```javascript
model_file=tf.train.latest_checkpoint('ckpt/')
saver.restore(sess,model_file)
```

一次 saver.save() 后可以在文件夹中看到新增的四个文件， 

![image-20200929102459806](https://i.loli.net/2020/09/29/kRYmSZn8BbwJ4NK.png)

 实际上每调用一次保存操作会创建后3个数据文件并创建一个检查点（checkpoint）文件，简单理解就是权重等参数被保存到 .ckpt.data 文件中，以字典的形式；图和元数据被保存到 .ckpt.meta 文件中，可以被 tf.train.import_meta_graph 加载到当前默认的图。

saver.restore()时填的文件名，因为在saver.save的时候，每个checkpoint会保存三个文件，如 `my-model-10000.meta`, `my-model-10000.index`, `my-model-10000.data-00000-of-00001`

在`import_meta_graph`时填的就是`meta`文件名，我们知道权值都保存在my-model-10000.data-00000-of-00001这个文件中，但是如果在restore方法中填这个文件名，就会报错，应该填的是前缀，这个前缀可以使用`tf.train.latest_checkpoint(checkpoint_dir)`这个方法获取。

下面代码是简单的保存和读取模型：（不包括加载图数据）

```javascript
import tensorflow as tf
import numpy as np
import os
 
#用numpy产生数据
x_data = np.linspace(-1,1,300)[:, np.newaxis] #转置
noise = np.random.normal(0,0.05, x_data.shape)
y_data = np.square(x_data)-0.5+noise
 
#输入层
x_ph = tf.placeholder(tf.float32, [None, 1])
y_ph = tf.placeholder(tf.float32, [None, 1])
 
#隐藏层
w1 = tf.Variable(tf.random_normal([1,10]))
b1 = tf.Variable(tf.zeros([1,10])+0.1)
wx_plus_b1 = tf.matmul(x_ph, w1) + b1
hidden = tf.nn.relu(wx_plus_b1)
 
#输出层
w2 = tf.Variable(tf.random_normal([10,1]))
b2 = tf.Variable(tf.zeros([1,1])+0.1)
wx_plus_b2 = tf.matmul(hidden, w2) + b2
y = wx_plus_b2
 
#损失
loss = tf.reduce_mean(tf.reduce_sum(tf.square(y_ph-y),reduction_indices=[1]))
train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
 
#保存模型对象saver
saver = tf.train.Saver()
 
#判断模型保存路径是否存在，不存在就创建
if not os.path.exists('tmp/'):
    os.mkdir('tmp/')
 
#初始化
with tf.Session() as sess:
    if os.path.exists('tmp/checkpoint'):         #判断模型是否存在
        saver.restore(sess, 'tmp/model.ckpt')    #存在就从模型中恢复变量
    else:
        init = tf.global_variables_initializer() #不存在就初始化变量
        sess.run(init)
 
    for i in range(1000):
        _,loss_value = sess.run([train_op,loss], feed_dict={x_ph:x_data, y_ph:y_data})
        if(i%50==0):
            save_path = saver.save(sess, 'tmp/model.ckpt')
            print("迭代次数：%d , 训练损失：%s"%(i, loss_value))
```

注：

- saver 的操作必须在 sess 建立后进行。
- model.ckpt 必须存在给定文件夹中，‘tmp/model.ckpt’ 这里至少要有一层文件夹，否则无法保存。
- 恢复模型时同保存时一样，是 ‘tmp/model.ckpt’，和那3个文件名都不一样。



如果不用`tf.train.latest_checkpoint（）`来自动获取最后一次保存的模型，则怎么做呢？



> [https://www.jianshu.com/p/7ebee4d10e49](https://www.jianshu.com/p/7ebee4d10e49)
>
> [https://blog.csdn.net/mylove0414/article/details/55097486](https://blog.csdn.net/mylove0414/article/details/55097486)
>



### Saver中的max_to_keep 参数



### keras中的TimeDistributed函数



> [https://blog.csdn.net/u012193416/article/details/79477220](https://blog.csdn.net/u012193416/article/details/79477220)
>
> [https://keras.io/zh/layers/wrappers/](https://keras.io/zh/layers/wrappers/)
>
> [https://blog.csdn.net/zh_JNU/article/details/85160379](https://blog.csdn.net/zh_JNU/article/details/85160379)
>
> [https://www.cnblogs.com/CheeseZH/p/13408658.html](https://www.cnblogs.com/CheeseZH/p/13408658.html)



### tf.concat()详解

```python
tf.concat([tensor1, tensor2, tensor3,...], axis)
# axis=0     代表在第0个维度拼接
# axis=1     代表在第1个维度拼接 
#axis=-1	 代表倒数第一个维度
```



> [https://blog.csdn.net/leviopku/article/details/82380118](https://blog.csdn.net/leviopku/article/details/82380118)







### shape

numpy数据的形状：

```python
 x.shape()
```



list 数据的形状：

```python
np.shape(x)
```



**注：**如果写`x.shape()` , 则会报错`ValueError: invalid literal for int() with base 10` 



torsor形状：

```python
x.get_shape()
```

### keras 的 fit函数

fit中以call()方法的形式来run session

> [https://blog.csdn.net/u012526436/article/details/102488164](https://blog.csdn.net/u012526436/article/details/102488164)



### Model 类继承



**可以通过继承 `Model` 类并在 `call` 方法中实现你自己的前向传播，以创建你自己的完全定制化的模型，**（`Model` 类继承 API 引入于 Keras 2.2.0）。

这里是一个用 `Model` 类继承写的简单的多层感知器的例子：

```python
import keras

class SimpleMLP(keras.Model):

    def __init__(self, use_bn=False, use_dp=False, num_classes=10):
        super(SimpleMLP, self).__init__(name='mlp')
        self.use_bn = use_bn
        self.use_dp = use_dp
        self.num_classes = num_classes

        self.dense1 = keras.layers.Dense(32, activation='relu')
        self.dense2 = keras.layers.Dense(num_classes, activation='softmax')
        if self.use_dp:
            self.dp = keras.layers.Dropout(0.5)
        if self.use_bn:
            self.bn = keras.layers.BatchNormalization(axis=-1)

    def call(self, inputs):
        x = self.dense1(inputs)
        if self.use_dp:
            x = self.dp(x)
        if self.use_bn:
            x = self.bn(x)
        return self.dense2(x)

model = SimpleMLP()
model.compile(...)
model.fit(...)
```

网络层定义在 `__init__(self, ...)` 中，前向传播在 `call(self, inputs)` 中指定。在 `call` 中，你可以指定自定义的损失函数，通过调用 `self.add_loss(loss_tensor)` （就像你在自定义层中一样）。

在类继承模型中，模型的拓扑结构是由 Python 代码定义的（而不是网络层的静态图）。这意味着该模型的拓扑结构不能被检查或序列化。因此，以下方法和属性**不适用于类继承模型**：

- `model.inputs` 和 `model.outputs`。
- `model.to_yaml()` 和 `model.to_json()`。
- `model.get_config()` 和 `model.save()`。

**关键点**：为每个任务使用正确的 API。`Model` 类继承 API 可以为实现复杂模型提供更大的灵活性，但它需要付出代价（比如缺失的特性）：它更冗长，更复杂，并且有更多的用户错误机会。如果可能的话，尽可能使用函数式 API，这对用户更友好。



> [https://blog.csdn.net/qq_27825451/article/details/90517036](https://blog.csdn.net/qq_27825451/article/details/90517036)



### 关于tensorflow的session、tensor、shape等基础知识（整理）

在tensorflow程序中，tensor只是占位符，在会话层没有run出tensor的值之前，我们是无法获知tensor的值的

> [https://blog.csdn.net/jiongnima/article/details/78524551](https://blog.csdn.net/jiongnima/article/details/78524551)
>
> [https://www.tensorflow.org/guide/tensor?hl=zh-cn](https://www.tensorflow.org/guide/tensor?hl=zh-cn)
>
> [https://www.jianshu.com/p/75a903a44cf2](https://www.jianshu.com/p/75a903a44cf2)



### tf.layers.flatten

在保留第0轴的情况下对输入的张量进行Flatten(扁平化)

代码示例：

```python
x=tf.placeholder(shape=(None,4,4),dtype='float32')

y=tf.layers.flatten(x)

print(y)
```

输出： 将后两维进行合并

```
Tensor("flatten/Reshape:0", shape=(?, 16), dtype=float32)
```



### tf.layers.dense

全连接层 ，相当于添加一个层。只**改变输入的最后一维**



### python - Tensorflow中 None，-1和？之间的区别

`None`表示未指定的维度。因此，如果您定义了一个占位符，您可以使用`None`来表示“这个维度可以有任何大小”。
占位符可以有多个`None`维度这仅仅意味着多个维度可以是不同的大小甚至整个形状都可以`None`来指定未知的维数。
`-1`是TensorFlow的一条指令，用于自行推断维度的大小。在`tf.reshape(input, [-1, input_size])`中，这意味着“重塑它，使第二个维度`input_size`，第一个维度是匹配元素总数所需的任何内容”。
这并不一定意味着维数是未知的，因为对于`None`如果输入张量的已知大小为10个元素，并且将其重塑为`[-1, 2]`，则张量流能够推断出完整的形状`[5, 2]`。
`-1`纯粹是为了方便。你可以把形状写下来，而不是让Tensorflow推断出来`None`另一方面，对于接受可变大小张量是必要的。
一个形状中只能有一个`-1`。多个是没有意义的，因为不可能推断出形状。例如，如果一个张量中有12个元素，则未定义将其重塑为`[-1, -1, 2]`——我们是否应该这样做？`[3, 2, 2]`？`[2, 3, 2]`？…
最后，问号正是tensorflow在打印张量和/或其形状时用来标记“未知”维度的内容。您发布的示例实际上会产生语法错误——您不能自己使用问号。未知维度的原因当然可以是具有`[6, 1, 2]`维度的占位符，并且通常根据占位符定义的张量（即应用于它们的某些运算的结果）也将具有未知维度。此外，有些操作可能没有指定（部分）它们的输出形状，这也可能导致未知。
这里可能还有一些我遗漏的技术细节，但根据经验：使用`None`作为占位符，使用`None`进行整形。这应该涵盖大多数用例。



> `？`== `None` ，维度是未知的
>
> `-1`代表根据推断之后的维度
>
> `(3,)`  表明张量是一个一维数组，这个数组的长度为3
>
> 



> [https://www.coder.work/article/2032326](https://www.coder.work/article/2032326)



### keras的 call 函数、build 函数

build() 用来初始化定义weights, 这里可以用父类的self.add_weight() 函数来初始化数据, 该函数必须将 self.built 设置为True, 以保证该 Layer 已经成功 build , 通常如上所示, 使用 super(MyLayer, self).build(input_shape) 来完成。

call() 用来执行 Layer 的职能, x就是该层的输入，x与权重kernel做点积，生成新的节点层，即当前 Layer 所有的计算过程均在该函数中完成。



`__init__()`和`build()`函数都在对Layer进行初始化，都初始化了一些成员函数

`__init__()`：保存成员变量的设置

`build()`：在`call()`函数第一次执行时会被调用一次，这时候可以知道输入数据的`shape`。返回去看一看，果然是`__init__()`函数中只初始化了输出数据的`shape`，而输入数据的`shape`需要在`build()`函数中动态获取，这也解释了为什么在有`__init__()`函数时还需要使用`build()`函数



`call()`函数则是在该layer被调用时执行。 



> [https://blog.csdn.net/qq_32623363/article/details/104128497](https://blog.csdn.net/qq_32623363/article/details/104128497)
>
> [https://blog.csdn.net/qq_27825451/article/details/90517036](https://blog.csdn.net/qq_27825451/article/details/90517036)



### tf.expand_dims（）

```python
tf.expand_dims(input, dim, name=None) #在指定位置增加维度
```

> [https://blog.csdn.net/jasonzzj/article/details/60811035](https://blog.csdn.net/jasonzzj/article/details/60811035)



### tf.boolean_mask（）

选择张量的特定维度的值

```python
tf.boolean_mask(tensor,mask,name='boolean_mask',axis=None)

#1-D example
tensor = [0, 1, 2, 3]
mask = np.array([True, False, True, False])
boolean_mask(tensor, mask)  # [0, 2]

#2-D example
tensor = [[1, 2], [3, 4], [5, 6]]
mask = np.array([True, False, True])
boolean_mask(tensor, mask)  # [[1, 2], [5, 6]]
```

> [https://blog.csdn.net/wuguangbin1230/article/details/81334544](https://blog.csdn.net/wuguangbin1230/article/details/81334544)





## 🚀pytorch

### PyTorch  torch.nn.Parameter()

**作用**：对于`self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))`，也就是将一个不可训练的类型`Tensor`转换成可以训练的类型`parameter`并将这个`parameter`绑定到这个`module`里面(`net.parameter()`中就有这个绑定的`parameter`，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个`self.v`变成了模型的一部分，成为了模型中根据训练可以改动的参数了。

使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。



> [https://www.jianshu.com/p/d8b77cc02410](https://www.jianshu.com/p/d8b77cc02410)



### PyTorch  nn.Linear（）

用于设置网络中的**全连接层的**

> [https://blog.csdn.net/qq_42079689/article/details/102873766](https://blog.csdn.net/qq_42079689/article/details/102873766)



### pytorch  nn.embedding() 词向量

词嵌入在 pytorch 中非常简单，只需要调用 `torch.nn.Embedding(m, n)` 就可以了，m 表示单词的总数目，n 表示词嵌入的维度，其实词嵌入就相当于是一个大矩阵，矩阵的每一行表示一个单词。

**随机初始化**

> [https://blog.csdn.net/david0611/article/details/81090371](https://blog.csdn.net/david0611/article/details/81090371)

### pytorch  torch.mean()

**torch.mean(input, dim, keepdim=False, out=None)**

返回新的张量，其中包含输入张量input指定维度dim中每行的平均值。

若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。

**参数：**

- input (Tensor) - 输入张量
- dim (int) - 指定进行均值计算的维度
- keepdim (bool, optional) - 输出张量是否保持与输入张量有相同数量的维度
- out (Tensor) - 结果张量

**例子：**

> a = torch.randn(4, 5)
> a
> 0.3168 0.4953 -0.6758 -0.5559 -0.6906
> 0.2241 2.2450 1.5735 -1.3815 -1.5199
> 0.0033 0.5236 -0.9070 -0.5961 -2.1281
> 0.9605 1.5314 -0.6555 -1.2584 -0.4160
> [torch.FloatTensor of size 4x5]
> torch.mean(a, 1, True)
> -0.2220
> 0.2283
> -0.6209
> 0.0324
> [torch.FloatTensor of size 4x1]



### np.triu() & np.tril()

```python
def triu（m， k）：
#取上三角阵  
#m：表示一个矩阵
#K：表示对角线的起始位置（k取值默认为0）

#k=0表示正常的上三角矩阵
#k=-1表示对角线的位置下移1个对角线
#k=1表示对角线的位置上移1个对角线

#同理，np.tril取下三角阵  
```

> [https://blog.csdn.net/weixin_37724529/article/details/102881776](https://blog.csdn.net/weixin_37724529/article/details/102881776)



###  pytorch  forward的使用以及原理   --pytorch使用

> [https://blog.csdn.net/u011501388/article/details/84062483](https://blog.csdn.net/u011501388/article/details/84062483)



### PyTorch  torch.nn.Parameter()详解

> [https://cloud.tencent.com/developer/article/1608348](https://cloud.tencent.com/developer/article/1608348)



### pytorch  view()

PyTorch中**view**函数作用为重构张量的维度

> torch.view(参数a,参数b,.....)，其中参数a=3,参数b=2决定了将一维的tt1重构成3*2维的张量。
>  有时候会出现torch.view(-1)或者torch.view(参数a,-1)这种情况。则-1参数是需要估算的。

```python
import torch
tt3=torch.tensor([[-0.3623,-0.6115],[0.7283,0.4699],[2.3261,0.1599]])
result2=tt3.view(2,-1).contiguous()
```

则`result2`为

```css
tensor([[-0.3623, -0.6115,  0.7283],
        [ 0.4699,  2.3261,  0.1599]])
```



### pytorch  model.parameters()

这个方法会获得模型的参数信息 。 

model.parameters()方法**返回的是一个生成器generator，每一个元素是从开头到结尾的参数**，parameters没有对应的key名称，是一个由纯参数组成的generator，查看Module的参数信息，**用于更新参数，或者用于模型的保存。**



> [https://blog.csdn.net/qq_27825451/article/details/95888267](https://blog.csdn.net/qq_27825451/article/details/95888267)



### pytorch  torch.optim.lr_scheduler

用于设置学习率的衰减

> [https://zhuanlan.zhihu.com/p/69411064](https://zhuanlan.zhihu.com/p/69411064)



### pytorch  torch.gather（）

例子

```python
b = torch.Tensor([[1,2,3],[4,5,6]])
print b
index_1 = torch.LongTensor([[0,1],[2,0]])
index_2 = torch.LongTensor([[0,1,1],[0,0,0]])
print torch.gather(b, dim=1, index=index_1)
print torch.gather(b, dim=0, index=index_2)

```

观察它的输出结果：

```
 1  2  3
 4  5  6
[torch.FloatTensor of size 2x3]


 1  2
 6  4
[torch.FloatTensor of size 2x2]


 1  5  6
 1  2  3
[torch.FloatTensor of size 2x3]


```

这里是官方文档的解释

```
torch.gather(input, dim, index, out=None) → Tensor

    Gathers values along an axis specified by dim.

    For a 3-D tensor the output is specified by:

    out[i][j][k] = input[index[i][j][k]][j][k]  # dim=0
    out[i][j][k] = input[i][index[i][j][k]][k]  # dim=1
    out[i][j][k] = input[i][j][index[i][j][k]]  # dim=2

    Parameters:	

        input (Tensor) – The source tensor
        dim (int) – The axis along which to index
        index (LongTensor) – The indices of elements to gather
        out (Tensor, optional) – Destination tensor

    Example:

    >>> t = torch.Tensor([[1,2],[3,4]])
    >>> torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]]))
     1  1
     4  3
    [torch.FloatTensor of size 2x2]

```

 :star: 可以看出，gather的作用是这样的，index实际上是索引，具体是行还是列的索引要看前面dim 的指定，比如对于我们的栗子，【1,2,3;4,5,6,】，指定dim=1，也就是横向，那么索引就是列号。index的大小就是输出的大小，所以比如index是【1,0;0,0】，那么看index第一行，1列指的是2， 0列指的是1，同理，第二行为4，4 。这样就输入为【2,1;4,4】，参考这样的解释看上面的输出结果，即可理解gather的含义。

gather在one-hot为输出的多分类问题中，可以把最大值坐标作为index传进去，然后提取到每一行的正确预测结果，这也是gather可能的一个作用。

> [https://blog.csdn.net/edogawachia/article/details/80515038](https://blog.csdn.net/edogawachia/article/details/80515038)
>
> [https://www.cnblogs.com/HongjianChen/p/9451526.html](https://www.cnblogs.com/HongjianChen/p/9451526.html)



### pytorch  损失函数NLLLoss和CrossEntropyLoss ()



> [https://blog.csdn.net/qq_22210253/article/details/85229988](https://blog.csdn.net/qq_22210253/article/details/85229988)
>
> [https://zhuanlan.zhihu.com/p/98785902](https://zhuanlan.zhihu.com/p/98785902)



### pytorch enisum函数 

> [https://www.cnblogs.com/mengnan/p/10319701.html](https://www.cnblogs.com/mengnan/p/10319701.html)

