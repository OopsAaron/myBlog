---
title: 2020-11-11-论文分享
mathjax: true
categories: 论文分享
date: 2020-11-11 15:04:42
tags:
top:
description:  |
	记录值得分享的论文;
	《ALBERT》: 一个轻量级的BERT
---



![image-20201111153229465](E:\myBlog\source\_posts\image-20201111153229465.png)



> ICLR 2020
>
> code url (official  tf) : [https://github.com/google-research/ALBERT](https://github.com/google-research/ALBERT)
>
> code url  (unofficial torch): [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)
>
> cited ：646



### 问题

BERT参数量有点多，消耗算力较大，能否去压缩BERT模型，构建一个轻量级的BERT？



### 解决

提出两个降低参数的策略 + 自监督损失（SOP）



#### 降低参数

降低参数1：factorized embedding parameterization ，将大型的token 嵌入矩阵分解为两个小矩阵。使得

降低参数2：cross-layer parameter sharing ， 以防随着层数加深参数增加 。 之前的参数共享策略只是关注于transformer这种的encoder-decoder模型中而不是BERT这种预训练-微调模型中。

这两个策略在减少大量参数且提高参数效率的同时，对BERT的性能影响不大

具体参考如下：

![image-20201121140455423](E:\myBlog\source\_posts\image-20201121140455423.png)



#### 自损失监督(SOP)

代替BERT中的NSP策略，使得模型更加关注句子内的一致性



#### 结果

模型ALBERT：提高训练速度，减少内存消耗

在语言理解任务上达到SOTA水平，具体参考如下：

![image-20201121142012520](E:\myBlog\source\_posts\image-20201121142012520.png)



### 模型

#### config 设置

ALBERT模型的config设置和BERT基本一致

![](E:\myBlog\source\_posts\image-20201121143009555.png)





#### factorized embedding parameterization

之前的BERT模型都是Embedding size = hiden size ， （是次优的）

![image-20201122094115198](E:\myBlog\source\_posts\image-20201122094115198.png)

![image-20201122094556848](E:\myBlog\source\_posts\image-20201122094556848.png)





### 实验







### 总结






