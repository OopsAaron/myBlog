---
title: 2020-07-30-pytorch使用手册
date: 2020-07-30 17:26:32
tags: pytorch
categories: pytorch
description: 记录在阅读pytorch代码时遇到的知识点 
---



###  **python中对于对象的拷贝分为浅拷贝(copy)和深拷贝(deepcopy)两种方式。其中浅拷贝由“=”完成。而深拷贝由copy模块中deepcopy()函数担任。**

###   ***\*浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。\****

### **1.浅拷贝"="的使用**

```python
#1.使用=复制不可变对象的值，以及复制以后修改其值后的变化。



val1 = 1000



val2 = val1



print("val1 is :{0},val2 is :{1}".format(val1,val2))#val1 is :1000,val2 is :1000



print(id(val1),id(val2))  #34052192 34052192



#这时候修改val1的值，尽管val2指向val1.但因为val1是不可变类型，修改其值，会重新给新值分配内存，然后指向他。



val1 += 1



print(val1,id(val1),val2,id(val2)) #1001 10131616 1000 10131568  值不一样，内存地址也不一样了



 



#1.使用=复制可变对象的值，以及复制以后修改其值后的变化。



ls1 =[1,2,3,4]



ls2 = ls1



print(id(ls1),id(ls2)) #43702792 43702792 直接使用=复制变量，内存地址一样，值也一样。



print(ls1,ls2) #[1, 2, 3, 4] [1, 2, 3, 4]直接使用=复制变量，内存地址一样，值也一样。



#这时候修改可变对的值,因为其值可变，所以只需要在原内存地址上修改即可。



ls1.append(5)



print(id(ls1),id(ls2)) #可变对象修改其值，内存引用不变



print(ls1,ls2) #[1, 2, 3, 4, 5] [1, 2, 3, 4, 5] 因为两个变量的内存指向一样，所以值也一样。
```

### 2.深拷贝：copy.deepcopy()函数

```python
#1.使用copy.deepcopy()拷贝不可变对象的值，以及复制以后修改其值后的变化。



val1 = 1000



val2 = copy.deepcopy(val1)



print("val1 is :{0},val2 is :{1}".format(val1,val2))#val1 is :1000,val2 is :1000



print(id(val1),id(val2))  #33717408 33717408 对于不可变对象，深度拷贝内存地址没有修改。



 



val1 += 1



print(val1,id(val1),val2,id(val2)) #1001 33717904 1000 33717408



 



#1.使用copy.deepcopy()复制可变对象的值，以及复制以后修改其值后的变化。



ls1 =[1,2,3,4]



ls2 = copy.deepcopy(ls1)



print(id(ls1),id(ls2)) #34628472 34628712 注意对于可变对象深度拷贝后内存地址都修改了。



print(ls1,ls2) #[1, 2, 3, 4] [1, 2, 3, 4]



ls1.append(5)



print(id(ls1),id(ls2)) #34628472 34628712



print(ls1,ls2) #[1, 2, 3, 4, 5] [1, 2, 3, 4] #注意这个时候ls2的值没有随着ls1修改。
```

### 总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“=”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy()来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。

### 3.注意一个特殊的copy(),跟深浅拷贝都有区别，慎用。

1. copy.copy对于可变类型，会进行浅拷贝
2. copy.copy对于不可变类型，不会拷贝，仅仅是指向

```python
1.使用copy()拷贝不可变对象



val1 = 1000



val2 = copy.copy(val1)



print(val1,val2)##1000 1000



print(id(val1),id(val2))#8551568 8551568



2.使用copy（）拷贝可变对象



ls1 =[1,2,3,4]



ls2 = copy.copy(ls1)



ls1.append(5)



print(ls1,ls2)  #[1, 2, 3, 4, 5] [1, 2, 3, 4]



 



看上去copy()函数效果和deepcopy()效果一样，可变对象拷贝后值也没有随着一个对象的修改而修改。



然后真实情况真是这样嘛？请看下面的案例，同样是拷贝可变对象。



 



origin = [1, 2, [3, 4]]



cop1 = copy.copy(origin)



cop2 = copy.deepcopy(origin)



origin[2][0] = "hey!"  #修改数据源的值



print(cop1,cop2) #[1, 2, ['hey!', 4]] [1, 2, [3, 4]]



 



很显然这时copy（）函数拷贝的值随着原对象的值修改了，而deepcopy()的值没有随着原对象的值修改。



主要是因为deepcopy会将复杂对象的每一层复制一个单独的个体出来对于copy（）函数要慎用，慎用。
```







神经网络的典型处理如下所示：

### **1. 定义可学习参数的网络结构（堆叠各层和层的设计）； 2. 数据集输入； 3. 对输入进行处理（由定义的网络层进行处理）,主要体现在网络的前向传播； 4. 计算loss ，由Loss层计算； 5. 反向传播求梯度； 6. 根据梯度改变参数值,最简单的实现方式（SGD）为:**   weight = weight - learning_rate * gradient

下面是利用PyTorch定义深度网络层（Op）示例：

```python
class FeatureL2Norm(torch.nn.Module):



    def __init__(self):



        super(FeatureL2Norm, self).__init__()



 



    def forward(self, feature):



        epsilon = 1e-6



#        print(feature.size())



#        print(torch.pow(torch.sum(torch.pow(feature,2),1)+epsilon,0.5).size())



        norm = torch.pow(torch.sum(torch.pow(feature,2),1)+epsilon,0.5).unsqueeze(1).expand_as(feature)



        return torch.div(feature,norm)
class FeatureRegression(nn.Module):



    def __init__(self, output_dim=6, use_cuda=True):



        super(FeatureRegression, self).__init__()



        self.conv = nn.Sequential(



            nn.Conv2d(225, 128, kernel_size=7, padding=0),



            nn.BatchNorm2d(128),



            nn.ReLU(inplace=True),



            nn.Conv2d(128, 64, kernel_size=5, padding=0),



            nn.BatchNorm2d(64),



            nn.ReLU(inplace=True),



        )



        self.linear = nn.Linear(64 * 5 * 5, output_dim)



        if use_cuda:



            self.conv.cuda()



            self.linear.cuda()



 



    def forward(self, x):



        x = self.conv(x)



        x = x.view(x.size(0), -1)



        x = self.linear(x)



        return x
```

由上例代码可以看到，不论是在定义网络结构还是定义网络层的操作（Op），均需要定义forward函数，下面看一下[PyTorch官网](https://pytorch.org/docs/stable/nn.html)对PyTorch的forward方法的描述：

![img](https://img-blog.csdnimg.cn/20181114105426553.PNG)

那么调用forward方法的具体流程是什么样的呢？[具体流程是这样的：](https://blog.csdn.net/u012436149/article/details/70145598)

### 以一个Module为例： **1. 调用module的call方法 2. module的call里面调用module的forward方法 3. forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下 4. 调用Function的call方法 5. Function的call方法调用了Function的forward方法。 6. Function的forward返回值 7. module的forward返回值 8. 在module的call进行forward_hook操作，然后返回值。**

上述中“调用module的call方法”是指nn.Module 的__call__方法。定义__call__方法的类可以当作函数调用，具体参考Python的面向对象编程。也就是说，当把定义的网络模型model当作函数调用的时候就自动调用定义的网络模型的forward方法。nn.Module 的__call__方法部分源码如下所示：

```python
def __call__(self, *input, **kwargs):



   result = self.forward(*input, **kwargs)



   for hook in self._forward_hooks.values():



       #将注册的hook拿出来用



       hook_result = hook(self, input, result)



   ...



   return result
```

可以看到，当执行model(x)的时候，底层自动调用forward方法计算结果。具体示例如下：

```python
class LeNet(nn.Module):



    def __init__(self):



        super(LeNet, self).__init__()



 



	layer1 = nn.Sequential()



	layer1.add_module('conv1', nn.Conv(1, 6, 3, padding=1))



	layer1.add_moudle('pool1', nn.MaxPool2d(2, 2))



	self.layer1 = layer1



 



	layer2 = nn.Sequential()



	layer2.add_module('conv2', nn.Conv(6, 16, 5))



	layer2.add_moudle('pool2', nn.MaxPool2d(2, 2))



	self.layer2 = layer2



 



	layer3 = nn.Sequential()



	layer3.add_module('fc1', nn.Linear(400, 120))



	layer3.add_moudle('fc2', nn.Linear(120, 84))



	layer3.add_moudle('fc3', nn.Linear(84, 10))



	self.layer3 = layer3



    def forward(self, x):



	x = self.layer1(x)



	x = self.layer2(x)



	x = x.view(x.size(0), -1)



	x = self.layer3(x)



	return x
```

### **model = LeNet() y = model(x)**

如上则调用网络模型定义的forward方法。