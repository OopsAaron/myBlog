---
title: 2020-10-22-MAML论文
mathjax: true
date: 2020-10-22 15:32:48
tags:
top:
categories:
description: MAML论文
---



### Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks



> ICML 2017
>
> authors ： Chelsea Finn，Pieter Abbeel， Sergey Levine （University of California ,Berkeley； Open AI）
>
> code url (official  tf) :  [https://github.com/cbfinn/maml](https://github.com/cbfinn/maml)
>
> code url (unofficial torch): [https://github.com/dragen1860/MAML-Pytorch]( https://github.com/dragen1860/MAML-Pytorch)
>
> 被引用次数：2427 



#### 背景

解决小样本学习问题很有挑战 - > 利用元学习的方法框架

元学习学习到一个模型，这个模型可以在少量新数据中快速学习。



#### 问题



前人通过学习update function或learning rule的训练方法，需要通过扩充模型的参数量或是限制模型结构（如限定RNN网络）等手段来提高准确率。



#### 解决

model-agnostic：模型无关。

MAML可以认为是一个框架，提供一个meta-learner用于训练base-learner。这里的meta-learner即MAML的精髓所在，用于 learning to learn；而base-learner则是在目标数据集上被训练，并实际用于预测任务的真正的数学模型。

绝大多数深度学习模型都可以作为base-learner无缝嵌入MAML中，而MAML甚至可以用于强化学习中，这就是MAML中model-agnostic的含义



本文的想法是训练一组初始化参数，**通过在初始参数的基础上进行一或多步的梯度调整，来达到仅用少量数据并且一次或几次的梯度更新就能快速适应新task的目的（能够有好的表现，小loss）**。



为了达到这一目的，训练模型需要最大化新task的loss function的参数敏感度（*maximizing the sensitivity of the loss functions of new tasks with respect to the parameters*），当敏感度提高时，极小的参数（参数量）变化也可以对模型带来较大的改进。

#### 贡献

本文提出的算法可以被轻松地使用在全连接网络、卷积网络以及递归神经网络中，并且可以使用多种loss函数，可以适用于多个领域，包括少样本的回归、图像分类，以及强化学习，并且使用更少的参数量达到了当时（2017年）最先进的专注于少样本分类领域的网络的准确率。



#### 模型



##### 算法图解

![image-20201103104804241](E:\myBlog\source\_posts\image-20201103104804241.png)





既然希望使用训练好的meta-learner仅通过几步梯度迭代便可适用于新的task，作者便将目标设定为，通过梯度迭代，找到对于task敏感的参数 θ 。

训练完成后的模型具有对新task的学习域分布最敏感的参数，因此可以在仅一或多次的梯度迭代中获得最符合新任务的 θ* ，达到较高的准确率。



##### 算法流程



<img src="E:\myBlog\source\_posts\image-20201103105129835.png" alt="image-20201103105129835" style="zoom:67%;" />

表示的是MAML预训练阶段的算法

gradient through a gradient 



##### 损失函数

梯度的计算是需要确定loss function的，MAML中loss根据不同的问题处理有不同的选择：

对于可监督回归问题，采用MSE 

对于可监督分类问题，采用交叉熵 



总的损失函数计算

![image-20201103135150758](E:\myBlog\source\_posts\image-20201103135150758.png)









<img src="E:\myBlog\source\_posts\image-20201103105239355.png" alt="image-20201103105239355" style="zoom: 67%;" />



~~为什么在support set 中梯度只更新一次？~~

1. 增加速度
2. 因为在实际的应用中
3. 小样本学习中



#### 实验



![image-20201103105805219](E:\myBlog\source\_posts\image-20201103105805219.png)



 first order approximation (一阶近似）：泰勒展开式对函数展开后，取前两项 ，和原始的不进行一阶近似的差别不大



#### 参考

> [https://zhuanlan.zhihu.com/p/57864886](https://zhuanlan.zhihu.com/p/57864886)
>
> [https://zhuanlan.zhihu.com/p/72920138](https://zhuanlan.zhihu.com/p/72920138)
>
> 



Learning to Compare: Relation Network for Few-Shot Learning

应用领域： 图像分类

使用方法：两个模块： embedding  module (CNNEncoder 普通的四层卷积) 和 relation module (RelationNetwork)



![image-20201023150506263](E:\myBlog\source\_posts\image-20201023150506263.png)

思路2： transformer可以嵌入到function函数中，作为特征提取器

思路3： 结合使用



Adaptive Subspaces for Few-Shot Learning

![image-20201023150053311](E:\myBlog\source\_posts\image-20201023150053311.png)



![image-20201023150105936](E:\myBlog\source\_posts\image-20201023150105936.png)

![image-20201023144851908](E:\myBlog\source\_posts\image-20201023144851908.png)