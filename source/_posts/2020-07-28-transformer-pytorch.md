---
title: 2020-07-28-transformerè§£è¯»-pytorchç‰ˆæœ¬
date: 2020-07-28 16:43:03
tags: transformer
categories: transformer
top: 100
description: transformer-pytorch
---

### å‰è¨€

æœ€è¿‘å‡ å¤©éƒ½åœ¨é˜…è¯»å“ˆä½›pytorchå®ç°transformerçš„ä»£ç ï¼Œä»£ç é£æ ¼å¾ˆå¥½ï¼Œå¾ˆå€¼å¾—å‚è€ƒå’Œç ”è¯»ã€‚å’Œå®éªŒå®¤å¸ˆå…„åˆåœ¨ä¸€èµ·è®¨è®ºäº†å‡ æ¬¡ï¼Œä»£ç æ€è·¯å’Œå®ç°è¿‡ç¨‹åŸºæœ¬éƒ½äº†è§£äº†ï¼Œå¯¹äºåŸè®ºæ–‡ [â€œAttention is All You Needâ€](https://arxiv.org/abs/1706.03762) ä¸­å…³äºtransformeræ¨¡å‹çš„ç†è§£åˆæ·±å…¥äº†è®¸å¤šã€‚æœç„¶è¦æƒ³äº†è§£æ¨¡å‹ï¼Œè¿˜æ˜¯è¦å¥½å¥½ç ”è¯»å®ç°ä»£ç ã€‚ä»¥ä¾¿äºåé¢è‡ªå·±ç»“åˆæ¨¡å‹çš„ç ”ç©¶ã€‚s

æœ¬ç¯‡æ˜¯å¯¹å®ç°ä»£ç çš„æ³¨é‡Šï¼ŒåŠ ä¸Šäº†è‡ªå·±çš„ç†è§£ï¼Œä¹Ÿä¼šæœ‰ä¸€äº›å‡½æ•°çš„ä»‹ç»æ‰©å……ã€‚



#### å‚è€ƒé“¾æ¥

> è§£è¯»çš„æ˜¯å“ˆä½›çš„ä¸€ç¯‡transformerçš„pytorchç‰ˆæœ¬å®ç°
>
> http://nlp.seas.harvard.edu/2018/04/03/attention.html
>
> å‚è€ƒå¦ä¸€ç¯‡åšå®¢
>
> http://fancyerii.github.io/2019/03/09/transformer-codes/
>
> Transformeræ³¨è§£åŠPyTorchå®ç°ï¼ˆä¸Šï¼‰
>
> https://www.jiqizhixin.com/articles/2018-11-06-10
>
> Transformeræ³¨è§£åŠPyTorchå®ç°ï¼ˆä¸‹ï¼‰
>
> https://www.jiqizhixin.com/articles/2018-11-06-18
>
> è®­ç»ƒè¿‡ç¨‹ä¸­çš„ Maskå®ç°
>
> https://www.cnblogs.com/wevolf/p/12484972.html
>
> transformerç»¼è¿°
>
> [https://libertydream.github.io/2020/05/03/Transformer-%E7%BB%BC%E8%BF%B0/](https://libertydream.github.io/2020/05/03/Transformer-ç»¼è¿°/)





### The Annotated Transformer



![è¿™æ˜¯ä¸€å¼ å›¾ç‰‡](https://i.loli.net/2020/07/28/NUAyXWJ5DzHmjuv.png)





```python
# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn 
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable
import matplotlib.pyplot as plt
import seaborn
seaborn.set_context(context="talk")

```





Transformerä½¿ç”¨äº†Self-Attentionæœºåˆ¶ï¼Œå®ƒåœ¨ç¼–ç æ¯ä¸€è¯çš„æ—¶å€™éƒ½èƒ½å¤Ÿæ³¨æ„(attend to)æ•´ä¸ªå¥å­ï¼Œä»è€Œå¯ä»¥è§£å†³é•¿è·ç¦»ä¾èµ–çš„é—®é¢˜ï¼ŒåŒæ—¶è®¡ç®—Self-Attentionå¯ä»¥ç”¨çŸ©é˜µä¹˜æ³•ä¸€æ¬¡è®¡ç®—æ‰€æœ‰çš„æ—¶åˆ»ï¼Œå› æ­¤å¯ä»¥å……åˆ†åˆ©ç”¨è®¡ç®—èµ„æº(CPU/GPUä¸Šçš„çŸ©é˜µè¿ç®—éƒ½æ˜¯å……åˆ†ä¼˜åŒ–å’Œé«˜åº¦å¹¶è¡Œçš„)ã€‚

### æ¨¡å‹ç»“æ„

Most competitive neural sequence transduction models have an encoder-decoder structure [(cite)](https://arxiv.org/abs/1409.0473). Here, `the encoder maps an input sequence of symbol representations (x1,â€¦,xn)(x1,â€¦,xn) to a sequence of continuous representations z=(z1,â€¦,zn)z=(z1,â€¦,zn). Given z, the decoder then generates an output sequence (y1,â€¦,ym)(y1,â€¦,ym) of symbols one element at a time.` At each step the model is auto-regressive [(cite)](https://arxiv.org/abs/1308.0850), consuming the previously generated symbols as additional input when generating the next.



**EncoderDecoderå®šä¹‰äº†ä¸€ç§é€šç”¨çš„Encoder-Decoderæ¶æ„**ï¼Œå…·ä½“çš„Encoderã€Decoderã€src_embedã€target_embedå’Œgeneratoréƒ½æ˜¯æ„é€ å‡½æ•°ä¼ å…¥çš„å‚æ•°ã€‚è¿™æ ·æˆ‘ä»¬**åšå®éªŒæ›´æ¢ä¸åŒçš„ç»„ä»¶å°±ä¼šæ›´åŠ æ–¹ä¾¿**ã€‚

```python
class EncoderDecoder(nn.Module): #å®šä¹‰çš„æ˜¯æ•´ä¸ªæ¨¡å‹ ï¼Œä¸åŒ…æ‹¬generator
    
    """
   æ ‡å‡†çš„Encoder-Decoderæ¶æ„ã€‚è¿™æ˜¯å¾ˆå¤šæ¨¡å‹çš„åŸºç¡€
    
    """
    """
    classé‡Œï¼Œ initå‡½æ•°æ˜¯å®ä¾‹åŒ–ä¸€ä¸ªå¯¹è±¡çš„æ—¶å€™ç”¨äºåˆå§‹åŒ–å¯¹è±¡ç”¨çš„
    forwardå‡½æ•°æ˜¯åœ¨æ‰§è¡Œè°ƒç”¨å¯¹è±¡çš„æ—¶å€™ä½¿ç”¨ï¼Œ éœ€è¦ä¼ å…¥æ­£ç¡®çš„å‚æ•° 
    åœ¨æ‰§è¡Œæ—¶å€™è°ƒç”¨__call__æ–¹æ³•ï¼Œç„¶åå†callé‡Œå†è°ƒç”¨forward
    """
    
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        # encoderå’Œdecoderéƒ½æ˜¯æ„é€ çš„æ—¶å€™ä¼ å…¥çš„ï¼Œè¿™æ ·ä¼šéå¸¸çµæ´»
        self.encoder = encoder
        self.decoder = decoder
        # æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„embeddingï¼ŒåŒ…æ‹¬embeddingå±‚å’Œposition encodeå±‚
        self.src_embed = src_embed #æºæ•°æ®é›†çš„åµŒå…¥
        self.tgt_embed = tgt_embed #ç›®æ ‡æ•°æ®é›†çš„åµŒå…¥ï¼Œä½œä¸ºdecoderçš„è¾“å…¥
        """
        generatoråé¢ä¼šè®²åˆ°ï¼Œå°±æ˜¯æ ¹æ®Decoderçš„éšçŠ¶æ€è¾“å‡ºå½“å‰æ—¶åˆ»çš„è¯
	    åŸºæœ¬çš„å®ç°å°±æ˜¯éšçŠ¶æ€è¾“å…¥ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œå…¨è¿æ¥å±‚çš„è¾“å‡ºå¤§å°æ˜¯è¯çš„ä¸ªæ•°
		ç„¶åæ¥ä¸€ä¸ªsoftmaxå˜æˆæ¦‚ç‡
        """
        self.generator = generator
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        #é¦–å…ˆè°ƒç”¨encodeæ–¹æ³•å¯¹è¾“å…¥è¿›è¡Œç¼–ç ï¼Œç„¶åè°ƒç”¨decodeæ–¹æ³•è§£ç 
        return self.decode(self.encode(src, src_mask), src_mask,
                            tgt, tgt_mask)
    
    def encode(self, src, src_mask):
        # è°ƒç”¨encoderæ¥è¿›è¡Œç¼–ç ï¼Œä¼ å…¥çš„å‚æ•°embeddingçš„srcå’Œsrc_mask
        return self.encoder(self.src_embed(src), src_mask)
    
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) #ç›®æ ‡æ˜¯è¾“å…¥çš„ä¸€éƒ¨åˆ†

    
class Generator(nn.Module):  #decoderåé¢çš„linear+softmax
    # æ ¹æ®Decoderçš„éšçŠ¶æ€è¾“å‡ºä¸€ä¸ªè¯
	# d_modelæ˜¯Decoderè¾“å‡ºçš„å¤§å°ï¼Œvocabæ˜¯è¯å…¸å¤§å° ï¼ˆæ•°æ®è¯­æ–™æœ‰å¤šå°‘è¯ ï¼‰
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab) #å…¨è¿æ¥ï¼Œä½œä¸ºsoftmaxçš„è¾“å…¥ã€‚

    def forward(self, x):
        return F.log_softmax(self.proj(x), dim=-1) #softmaxçš„logå€¼
```



æ³¨ï¼š`Generatorè¿”å›çš„æ˜¯softmaxçš„logå€¼`ã€‚åœ¨PyTorché‡Œä¸ºäº†è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œæœ‰ä¸¤ç§æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨**nn.CrossEntropyLoss()**ï¼Œä¸€ç§æ˜¯ä½¿ç”¨**NLLLoss()**ã€‚å¾ˆå¤šå¼€æºä»£ç é‡Œç¬¬äºŒç§æ›´å¸¸è§ï¼Œ

æˆ‘ä»¬å…ˆçœ‹CrossEntropyLossï¼Œå®ƒå°±æ˜¯è®¡ç®—äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œæ¯”å¦‚ï¼š

```python
criterion = nn.CrossEntropyLoss()

x = torch.randn(1, 5)
y = torch.empty(1, dtype=torch.long).random_(5)

loss = criterion(x, y)
```

æ¯”å¦‚ä¸Šé¢çš„ä»£ç ï¼Œå‡è®¾æ˜¯5åˆ†ç±»é—®é¢˜ï¼Œxè¡¨ç¤ºæ¨¡å‹çš„è¾“å‡ºlogits(batch=1)ï¼Œè€Œyæ˜¯çœŸå®åˆ†ç±»çš„ä¸‹æ ‡(0-4)ã€‚å®é™…çš„è®¡ç®—è¿‡ç¨‹ä¸ºï¼š<img src="https://i.loli.net/2020/08/06/KyPspa4Cqef6m8Q.png" alt="image-20200806000621448" style="zoom: 67%;" />

æ¯”å¦‚logitsæ˜¯[0,1,2,3,4]ï¼ŒçœŸå®åˆ†ç±»æ˜¯3ï¼Œé‚£ä¹ˆä¸Šå¼å°±æ˜¯ï¼š

<img src="https://i.loli.net/2020/08/06/i7mfUWAeHE5P1zd.png" alt="image-20200806000641945" style="zoom:67%;" />

å› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨NLLLoss()é…åˆF.log_softmaxå‡½æ•°(æˆ–è€…nn.LogSoftmaxï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå‡½æ•°è€Œæ˜¯ä¸€ä¸ªModuleäº†)æ¥å®ç°ä¸€æ ·çš„æ•ˆæœï¼š

```python
m = nn.LogSoftmax(dim=1)
criterion = nn.NLLLoss()
x = torch.randn(1, 5)
y = torch.empty(1, dtype=torch.long).random_(5)
loss = criterion(m(x), y)
```

NLLLoss(Negative Log Likelihood Loss)æ˜¯è®¡ç®—è´Ÿlogä¼¼ç„¶æŸå¤±ã€‚å®ƒè¾“å…¥çš„xæ˜¯log_softmaxä¹‹åçš„ç»“æœ(é•¿åº¦ä¸º5çš„æ•°ç»„)ï¼Œyæ˜¯çœŸå®åˆ†ç±»(0-4)ï¼Œè¾“å‡ºå°±æ˜¯x[y]ã€‚å› æ­¤ä¸Šé¢çš„ä»£ç ä¸ºï¼š

```python
criterion(m(x), y)=m(x)[y]
```



The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.



![png](https://i.loli.net/2020/07/28/P3fSgRhrmFtlpxY.png)

### Encoder and Decoder Stacks

#### Encoder

Encoderå’ŒDecoderéƒ½æ˜¯ç”±Nä¸ªç›¸åŒç»“æ„çš„Layerå †ç§¯(stack)è€Œæˆã€‚**å› æ­¤æˆ‘ä»¬é¦–å…ˆå®šä¹‰cloneså‡½æ•°ï¼Œç”¨äºå…‹éš†ç›¸åŒçš„SubLayerã€‚**

è¿™é‡Œä½¿ç”¨äº†**nn.ModuleList**ï¼ŒModuleListå°±åƒä¸€ä¸ªæ™®é€šçš„Pythonçš„Listï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹æ ‡æ¥è®¿é—®å®ƒï¼Œå®ƒçš„å¥½å¤„æ˜¯ä¼ å…¥çš„ModuleListçš„æ‰€æœ‰Moduleéƒ½ä¼šæ³¨å†Œçš„PyTorché‡Œï¼Œè¿™æ ·Optimizerå°±èƒ½æ‰¾åˆ°è¿™é‡Œé¢çš„å‚æ•°ï¼Œä»è€Œèƒ½å¤Ÿç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°è¿™äº›å‚æ•°ã€‚ä½†æ˜¯nn.ModuleListå¹¶ä¸æ˜¯Module(çš„å­ç±»)ï¼Œå› æ­¤å®ƒæ²¡æœ‰forwardç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬é€šå¸¸æŠŠå®ƒæ”¾åˆ°æŸä¸ªModuleé‡Œã€‚

```python
def clones(module, N):  #å…‹éš†Nå±‚ï¼Œæ˜¯ä¸ªå±‚æ•°çš„åˆ—è¡¨ã€‚ copy.deepcopyæ˜¯æ·±å¤åˆ¶ï¼Œ ä¸€ä¸ªæ”¹å˜ä¸ä¼šå½±å“å¦ä¸€ä¸ª

	return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) #å¤åˆ¶N=6å±‚
```



```python
class Encoder(nn.Module):  #å®šä¹‰ç¼–ç å™¨ 
    
    #Encoderæ˜¯Nä¸ªEncoderLayerçš„stack
    def __init__(self, layer, N): # æ ¹æ®make_modelå®šä¹‰ï¼Œlayer = encoderlayer ï¼ˆsublayerï¼‰
        super(Encoder, self).__init__()
        self.layers = clones(layer, N) #ç¼–ç å™¨æœ‰6å±‚ç¼–ç å±‚ï¼Œæ ¹æ®ä¸Šè¿°å‡½æ•°çš„å®šä¹‰ï¼Œmodule=layer
        self.norm = LayerNorm(layer.size) #è°ƒç”¨ä¸‹é¢çš„LayerNormã€‚ åˆ†å¼€å®šä¹‰æ˜¯å› ä¸º LayerNorm = 2* layer
        
    def forward(self, x, mask): 
      	 #é€å±‚è¿›è¡Œå¤„ç†
        for layer in self.layers: # x åœ¨æ¯ä¸€å±‚ä¸­ä¼ é€’
            x = layer(x, mask)
        return self.norm(x) #æœ€ç»ˆencoderçš„è¿”å›å€¼
```



```python
class LayerNorm(nn.Module): #add & norméƒ¨åˆ†  ä½œä¸ºæ¯ä¸€ä¸ªå­å±‚çš„è¾“å‡º
    "Construct a layernorm module (See citation for details)."
    def __init__(self, features, eps=1e-6): #feature = layer.size layerçš„å½¢çŠ¶
        super(LayerNorm, self).__init__()
        
        self.a_2 = nn.Parameter(torch.ones(features))  #å°†åé¢çš„tensorè½¬æ¢ä¸ºå¯ä¼˜åŒ–çš„å‚æ•°
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps #å¾ˆå°çš„å€¼

    def forward(self, x): # å¹³å‡å€¼å’Œæ ‡å‡†å·®
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 #è¾“å‡º
```

**ä¸ç®¡æ˜¯Self-Attentionè¿˜æ˜¯å…¨è¿æ¥å±‚ï¼Œéƒ½é¦–å…ˆæ˜¯LayerNormï¼Œç„¶åæ˜¯Self-Attention/Denseï¼Œç„¶åæ˜¯Dropoutï¼Œæœ€å¥½æ˜¯æ®‹å·®è¿æ¥ã€‚è¿™é‡Œé¢æœ‰å¾ˆå¤šå¯ä»¥é‡ç”¨çš„ä»£ç ï¼Œæˆ‘ä»¬æŠŠå®ƒå°è£…æˆSublayerConnectionã€‚**

------

That is, `the output of each sub-layer is LayerNorm(x+Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.` We apply dropout [(cite)](http://jmlr.org/papers/v15/srivastava14a.html) to the output of each sub-layer, before it is added to the sub-layer input and normalized.

To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension `dmodel=512`.

```python
class SublayerConnection(nn.Module): #æ¯ä¸€ä¸ªç¼–ç å±‚ä¸­çš„ä¸¤ä¸ªå­å±‚ä¹‹é—´çš„è¿æ¥
    """
	LayerNorm + sublayer(Self-Attenion/Dense) + dropout + æ®‹å·®è¿æ¥
	ä¸ºäº†ç®€å•ï¼ŒæŠŠLayerNormæ”¾åˆ°äº†å‰é¢ï¼Œè¿™å’ŒåŸå§‹è®ºæ–‡ç¨æœ‰ä¸åŒï¼ŒåŸå§‹è®ºæ–‡LayerNormåœ¨æœ€åã€‚
	"""
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
       #sublayeræ˜¯ä¼ å…¥çš„å‚æ•°ï¼Œå‚è€ƒDecoderLayerï¼Œå®ƒå¯ä»¥å½“æˆå‡½æ•°è°ƒç”¨ï¼Œè¿™ä¸ªå‡½æ•°çš„æœ‰ä¸€ä¸ªè¾“å…¥å‚æ•°
        return x + self.dropout(sublayer(self.norm(x))) #è°ƒç”¨layernorm ï¼Œæ­£åˆ™åŒ–ä¹‹åå†ç›¸åŠ 
```

è¿™ä¸ªç±»ä¼šæ„é€ LayerNormå’ŒDropoutï¼Œä½†æ˜¯Self-Attentionæˆ–è€…Denseå¹¶ä¸åœ¨è¿™é‡Œæ„é€ ï¼Œè¿˜æ˜¯æ”¾åœ¨äº†EncoderLayeré‡Œï¼Œåœ¨forwardçš„æ—¶å€™ç”±EncoderLayerä¼ å…¥ã€‚è¿™æ ·çš„å¥½å¤„æ˜¯æ›´åŠ é€šç”¨ï¼Œæ¯”å¦‚Decoderä¹Ÿæ˜¯ç±»ä¼¼çš„éœ€è¦åœ¨Self-Attentionã€Attentionæˆ–è€…Denseå‰é¢ååŠ ä¸ŠLayerNormå’ŒDropoutä»¥åŠæ®‹å·®è¿æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¤ç”¨ä»£ç ã€‚ä½†æ˜¯è¿™é‡Œè¦æ±‚ä¼ å…¥çš„sublayerå¯ä»¥ä½¿ç”¨ä¸€ä¸ªå‚æ•°æ¥è°ƒç”¨çš„å‡½æ•°(æˆ–è€…æœ‰__call__)ã€‚



------



forwardè°ƒç”¨sublayer[0] (è¿™æ˜¯SublayerConnectionå¯¹è±¡)çš„__call__æ–¹æ³•ï¼Œæœ€ç»ˆä¼šè°ƒåˆ°å®ƒçš„forwardæ–¹æ³•ï¼Œè€Œè¿™ä¸ªæ–¹æ³•éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼Œ**ä¸€ä¸ªæ˜¯è¾“å…¥Tensorï¼Œä¸€ä¸ªæ˜¯ä¸€ä¸ªcallableï¼Œå¹¶ä¸”è¿™ä¸ªcallableå¯ä»¥ç”¨ä¸€ä¸ªå‚æ•°æ¥è°ƒç”¨**ã€‚è€Œ**self_attnå‡½æ•°éœ€è¦4ä¸ªå‚æ•°(Queryçš„è¾“å…¥,Keyçš„è¾“å…¥,Valueçš„è¾“å…¥å’ŒMask)**ï¼Œå› æ­¤è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨lambdaçš„æŠ€å·§æŠŠå®ƒå˜æˆä¸€ä¸ªå‚æ•°xçš„å‡½æ•°(maskå¯ä»¥çœ‹æˆå·²çŸ¥çš„æ•°)ã€‚

  Callable ç±»å‹æ˜¯å¯ä»¥è¢«æ‰§è¡Œè°ƒç”¨æ“ä½œçš„ç±»å‹ã€‚åŒ…å«è‡ªå®šä¹‰å‡½æ•°ç­‰ã€‚è‡ªå®šä¹‰çš„å‡½æ•°æ¯”å¦‚ä½¿ç”¨defã€lambdaæ‰€å®šä¹‰çš„å‡½æ•°

```python

class EncoderLayer(nn.Module): #æ¯ä¸€ä¸ªç¼–ç å±‚
    "Encoder is made up of self-attn and feed forward (defined below)"
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2) #æ¯ä¸€å±‚æœ‰2å­å±‚
        self.size = size

    def forward(self, x, mask):
      #attentionå±‚ï¼Œæ‹¬å·é‡Œé¢æ˜¯å‚æ•°ã€‚æ¥æ”¶æ¥è‡ªattentionçš„è¾“å‡º
    """
     lambda : atten()SublayerConnectioné‡Œæ˜¯ä½œä¸ºsublayerå‡ºç°çš„ï¼Œè€Œå®ƒçš„å‚æ•°æ˜¯norm(x),norm(x)çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå‘é‡xï¼Œ
   æ‰€ä»¥attençš„å‚æ•°æ˜¯åªæœ‰ä¸€ä¸ªxï¼Œ è€Œåœ¨muitiheadé‡Œé¢ï¼Œkã€qã€våœ¨å‡½æ•°é‡Œæ˜¯è¦è¢«é‡æ–°æ ¹æ®xè®¡ç®—çš„
    """
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) 
        return self.sublayer[1](x, self.feed_forward) #xæ˜¯atten+normä¹‹åçš„è¾“å‡ºï¼Œå†ffè¾“å‡º
    """
    å¯ä»¥ç†è§£ä¸º
    z = lambda y: self.self_attn(y, y, y, mask)
	x = self.sublayer[0](x, z)
    """
```



#### Decoder

The decoder is also composed of a stack of `N=6` identical layers.

```python
class Decoder(nn.Module):
    "Generic N layer decoder with masking."
    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
     #memory: ç¼–ç å™¨çš„è¾“å‡º xæ˜¯è¾“å…¥
    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
```



```python
class DecoderLayer(nn.Module): #æ¯ä¸€å±‚è§£ç å±‚
    "Decoder is made of self-attn, src-attn, and feed forward (defined below)"
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3) #æ¯ä¸€å±‚æœ‰3ä¸ªå­å±‚
        
    def forward(self, x, memory, src_mask, tgt_mask):
        "Follow Figure 1 (right) for connections."
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) #ç¬¬ä¸€å­å±‚
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) #ç¬¬äºŒå­å±‚ 
        return self.sublayer[2](x, self.feed_forward) #ç¬¬ä¸‰å­å±‚ 
```

**src-attnå’Œself-attnçš„å®ç°æ˜¯ä¸€æ ·çš„ï¼Œåªä¸è¿‡ä½¿ç”¨çš„Queryï¼ŒKeyå’ŒValueçš„è¾“å…¥ä¸åŒã€‚**æ™®é€šçš„Attention(src-attn)çš„Queryæ˜¯ä¸‹å±‚è¾“å…¥è¿›æ¥çš„(æ¥è‡ªself-attnçš„è¾“å‡º)ï¼ŒKeyå’ŒValueæ˜¯Encoderæœ€åä¸€å±‚çš„è¾“å‡ºmemoryï¼›è€ŒSelf-Attentionçš„Queryï¼ŒKeyå’ŒValueéƒ½æ˜¯æ¥è‡ªä¸‹å±‚è¾“å…¥è¿›æ¥çš„ã€‚





------

Decoderå’ŒEncoderæœ‰ä¸€ä¸ªå…³é”®çš„ä¸åŒï¼šDecoderåœ¨è§£ç ç¬¬tä¸ªæ—¶åˆ»çš„æ—¶å€™åªèƒ½ä½¿ç”¨**1â€¦tæ—¶åˆ»**çš„è¾“å…¥ï¼Œè€Œä¸èƒ½ä½¿ç”¨t+1æ—¶åˆ»åŠå…¶ä¹‹åçš„è¾“å…¥ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥äº§ç”Ÿä¸€ä¸ªMaskçŸ©é˜µï¼Œæ‰€ä»¥ä»£ç å¦‚ä¸‹ï¼š

æ³¨æ„ï¼š tæ—¶åˆ»åŒ…æ‹¬tæ—¶åˆ»çš„è¾“å…¥

```python
def subsequent_mask(size):  #å°†iåé¢çš„maskæ‰
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') #triu ä¸Šä¸‰è§’
    return torch.from_numpy(subsequent_mask) == 0 #å°†numpyæ ¼å¼è½¬æ¢ä¸ºtensoræ ¼å¼ï¼Œåˆ¤æ–­æ˜¯å¦ä¸º0ï¼Œ è¾“å‡ºå¸ƒå°”å€¼
```





![png](https://i.loli.net/2020/08/08/7brnPfDJxsLBtvh.png)

å®ƒçš„è¾“å‡ºï¼š

```
print(subsequent_mask(5))
# è¾“å‡º
  1  0  0  0  0
  1  1  0  0  0
  1  1  1  0  0
  1  1  1  1  0
  1  1  1  1  1
```

æˆ‘ä»¬å‘ç°å®ƒè¾“å‡ºçš„æ˜¯ä¸€ä¸ªæ–¹é˜µï¼Œå¯¹è§’çº¿å’Œä¸‹é¢éƒ½æ˜¯1ã€‚**ç¬¬ä¸€è¡Œåªæœ‰ç¬¬ä¸€åˆ—æ˜¯1ï¼Œå®ƒçš„æ„æ€æ˜¯æ—¶åˆ»1åªèƒ½attend toè¾“å…¥1**ï¼Œç¬¬ä¸‰è¡Œè¯´æ˜æ—¶åˆ»3å¯ä»¥attend to {1,2,3}è€Œä¸èƒ½attend to{4,5}çš„è¾“å…¥ï¼Œå› ä¸ºåœ¨çœŸæ­£Decoderçš„æ—¶å€™è¿™æ˜¯å±äºFutureçš„ä¿¡æ¯ã€‚ä»£ç é¦–å…ˆä½¿ç”¨triuäº§ç”Ÿä¸€ä¸ªä¸Šä¸‰è§’é˜µï¼š

```
0 1 1 1 1
0 0 1 1 1
0 0 0 1 1
0 0 0 0 1
0 0 0 0 0
```

ç„¶åéœ€è¦æŠŠ0å˜æˆ1ï¼ŒæŠŠ1å˜æˆ0ï¼Œè¿™å¯ä»¥ä½¿ç”¨ matrix == 0æ¥å®ç°ã€‚

å› ä¸ºï¼šå¸ƒå°”å€¼Trueè¢«ç´¢å¼•æ±‚å€¼ä¸º1ï¼Œè€ŒFalseå°±ç­‰äº0ã€‚



#### Attention

An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.

We call our particular attention â€œ`Scaled Dot-Product Attention`â€. The input consists of queries and keys of dimension `dk`, and values of dimension `dv`. We compute the dot products of the query with all keys, divide each by `âˆšdk`, and apply a softmax function to obtain the weights on the values.



![image-20200806015122441](https://i.loli.net/2020/08/06/O3UNSGF7Poa1w4Q.png)

**Attentionå¯ä»¥çœ‹æˆä¸€ä¸ªå‡½æ•°ï¼Œå®ƒçš„è¾“å…¥æ˜¯Query,Key,Valueå’ŒMaskï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªTensor**ã€‚å…¶ä¸­è¾“å‡ºæ˜¯Valueçš„åŠ æƒå¹³å‡ï¼Œè€Œæƒé‡æ¥è‡ªQueryå’ŒKeyçš„è®¡ç®—ã€‚å…·ä½“çš„è®¡ç®—å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š

<img src="https://i.loli.net/2020/07/28/WaSfHnNdt2L1AXU.png" alt="image-20200728212241453" style="zoom:50%;" />

```python
def attention(query, key, value, mask=None, dropout=None):
    "Compute 'Scaled Dot Product Attention'"
    d_k = query.size(-1) # query.sizeçš„æœ€åä¸€ç»´
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:# å¦‚æœæœ‰mask
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim = -1)
    if dropout is not None: #å¯¹p_attnè¿›è¡Œdropout
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn
```

æˆ‘ä»¬çŸ¥é“, åœ¨è®­ç»ƒçš„æ—¶å€™, æˆ‘ä»¬æ˜¯ä»¥ batch_size ä¸ºå•ä½çš„, é‚£ä¹ˆå°±ä¼šæœ‰ padding, ä¸€èˆ¬æˆ‘ä»¬å– pad == 0, é‚£ä¹ˆå°±ä¼šé€ æˆåœ¨ Attention çš„æ—¶å€™, query çš„å€¼ä¸º 0, query çš„å€¼ä¸º 0, æ‰€ä»¥æˆ‘ä»¬è®¡ç®—çš„å¯¹åº”çš„ scores çš„å€¼ä¹Ÿæ˜¯ 0, é‚£ä¹ˆå°±ä¼šå¯¼è‡´ softmax å¾ˆå¯èƒ½åˆ†é…ç»™è¯¥å•è¯ä¸€ä¸ªç›¸å¯¹ä¸æ˜¯å¾ˆå°çš„æ¯”ä¾‹, å› æ­¤, æˆ‘ä»¬å°† pad å¯¹åº”çš„ score å–å€¼ä¸º**è´Ÿæ— ç©·**ï¼ˆæ™®é€šçš„è®¡ç®—ï¼Œscoreå¯ä»¥ä¸ºè´Ÿæ•°ï¼Ÿï¼‰, ä»¥æ­¤æ¥å‡å° pad çš„å½±å“. 



å¾ˆå®¹æ˜“æƒ³åˆ°, åœ¨ decoder, **æœªé¢„æµ‹çš„å•è¯**ä¹Ÿæ˜¯ç”¨ padding çš„æ–¹å¼åŠ å…¥åˆ° batch çš„, æ‰€ä»¥ä½¿ç”¨çš„mask æœºåˆ¶ä¸ padding æ—¶mask çš„æœºåˆ¶æ˜¯ç›¸åŒçš„, æœ¬è´¨ä¸Šéƒ½æ˜¯query çš„å€¼ä¸º0, åªæ˜¯ mask çŸ©é˜µä¸åŒ, æˆ‘ä»¬å¯ä»¥æ ¹æ® decoder éƒ¨åˆ†çš„ä»£ç å‘ç°è¿™ä¸€ç‚¹.



------



æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª**å®é™…çš„ä¾‹å­è·Ÿè¸ªä¸€äº›ä¸åŒTensorçš„shape**ï¼Œç„¶åå¯¹ç…§å…¬å¼å°±å¾ˆå®¹æ˜“ç†è§£ã€‚æ¯”å¦‚**Qæ˜¯(30,8,33,64)ï¼Œå…¶ä¸­30æ˜¯batchï¼Œ8æ˜¯headä¸ªæ•°ï¼Œ33æ˜¯åºåˆ—é•¿åº¦ï¼Œ64æ˜¯æ¯ä¸ªæ—¶åˆ»çš„ç‰¹å¾æ•°ï¼ˆsizeï¼‰ã€‚Kå’ŒQçš„shapeå¿…é¡»ç›¸åŒçš„ï¼Œè€ŒVå¯ä»¥ä¸åŒï¼Œä½†æ˜¯è¿™é‡Œçš„å®ç°shapeä¹Ÿæ˜¯ç›¸åŒçš„ã€‚**

```python
	scores = torch.matmul(query, key.transpose(-2, -1)) \
	/ math.sqrt(d_k)
```

ä¸Šé¢çš„ä»£ç å®ç°<img src="https://i.loli.net/2020/08/06/rLCJ7VFBAsmQb4x.png" alt="image-20200806014945713" style="zoom:50%;" />ï¼Œå’Œå…¬å¼é‡Œç¨å¾®ä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œçš„Qå’ŒKéƒ½æ˜¯4dçš„Tensorï¼ŒåŒ…æ‹¬batchå’Œheadç»´åº¦ã€‚**matmulä¼šæŠŠqueryå’Œkeyçš„æœ€åä¸¤ç»´è¿›è¡ŒçŸ©é˜µä¹˜æ³•**ï¼Œè¿™æ ·æ•ˆç‡æ›´é«˜ï¼Œå¦‚æœæˆ‘ä»¬è¦ç”¨æ ‡å‡†çš„çŸ©é˜µ(äºŒç»´Tensor)ä¹˜æ³•æ¥å®ç°ï¼Œé‚£ä¹ˆéœ€è¦éå†batchç»´å’Œheadç»´ï¼š

```python
	batch_num = query.size(0) # query.size(0)è¿”å›çš„æ˜¯0ç»´çš„æ•°
	head_num = query.size(1)
	for i in range(batch_num):
		for j in range(head_num):
			scores[i,j] = torch.matmul(query[i,j], key[i,j].transpose())
```

è€Œä¸Šé¢çš„å†™æ³•ä¸€æ¬¡å®Œæˆæ‰€æœ‰è¿™äº›å¾ªç¯ï¼Œæ•ˆç‡æ›´é«˜ã€‚**è¾“å‡ºçš„scoreæ˜¯(30, 8, 33, 33)**ï¼Œå‰é¢ä¸¤ç»´ä¸çœ‹ï¼Œé‚£**ä¹ˆæ˜¯ä¸€ä¸ª(33, 33)çš„attentionçŸ©é˜µaï¼Œaijè¡¨ç¤ºæ—¶åˆ» iå…³æ³¨ j çš„å¾—åˆ†**(è¿˜æ²¡æœ‰ç»è¿‡softmaxå˜æˆæ¦‚ç‡)ã€‚

**åœ¨ç¼–ç å™¨çš„attentionä¸­src_maskçš„ä½œç”¨ï¼ï¼ï¼**

æ¥ä¸‹æ¥æ˜¯`scores.masked_fill(mask == 0, -1e9)`ï¼Œç”¨äº**æŠŠmaskæ˜¯0çš„å˜æˆä¸€ä¸ªå¾ˆå°çš„æ•°**ï¼Œè¿™æ ·åé¢ç»è¿‡softmaxä¹‹åçš„æ¦‚ç‡å°±å¾ˆæ¥è¿‘é›¶(ä½†æ˜¯ç†è®ºä¸Šè¿˜æ˜¯ç”¨æ¥å¾ˆå°‘ä¸€ç‚¹ç‚¹æœªæ¥çš„ä¿¡æ¯)ã€‚

> masked_fill_(mask, value)ï¼šæ©ç æ“ä½œ
> masked_fillæ–¹æ³•æœ‰ä¸¤ä¸ªå‚æ•°ï¼Œmaskeå’Œvalueï¼Œmaskæ˜¯ä¸€ä¸ªpytorchå¼ é‡ï¼ˆTensorï¼‰ï¼Œ**å…ƒç´ æ˜¯å¸ƒå°”å€¼ï¼Œvalueæ˜¯è¦å¡«å……çš„å€¼**ï¼Œå¡«å……è§„åˆ™æ˜¯maskä¸­å–å€¼ä¸ºTrueä½ç½®å¯¹åº”äºselfçš„ç›¸åº”ä½ç½®ç”¨valueå¡«å……ã€‚
>
> æ³¨ï¼šå‚æ•°maskå¿…é¡»ä¸scoreçš„sizeç›¸åŒæˆ–è€…ä¸¤è€…æ˜¯å¯å¹¿æ’­(broadcasting-semantics)çš„
>
> 
>
> pytorch masked_fillæ–¹æ³•ç®€å•ç†è§£ 
>
>  https://blog.csdn.net/jianyingyao7658/article/details/103382654
>
> pytorch å¹¿æ’­è¯­ä¹‰(Broadcasting semantics) 
>
>  https://blog.csdn.net/qq_35012749/article/details/88308657



è¿™é‡Œ**maskæ˜¯(30, 1, 1, 33)çš„tensor**ï¼Œå› ä¸º8ä¸ªheadçš„maskéƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€æœ‰ç¬¬äºŒç»´æ˜¯1ï¼Œmasked_fillæ—¶ä½¿ç”¨broadcastingå°±å¯ä»¥äº†ã€‚è¿™é‡Œæ˜¯self-attentionçš„maskï¼Œæ‰€ä»¥æ¯ä¸ªæ—¶åˆ»éƒ½å¯ä»¥attendåˆ°æ‰€æœ‰å…¶å®ƒæ—¶åˆ»ï¼Œæ‰€æœ‰ç¬¬ä¸‰ç»´ä¹Ÿæ˜¯1ï¼Œä¹Ÿä½¿ç”¨broadcastingã€‚å¦‚æœæ˜¯æ™®é€šçš„maskï¼Œé‚£ä¹ˆmaskçš„shapeæ˜¯(30, 1, 33, 33)ã€‚

è¿™æ ·è®²æœ‰ç‚¹æŠ½è±¡ï¼Œæˆ‘ä»¬å¯ä»¥ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å‡è®¾batch=2, head=8ã€‚ç¬¬ä¸€ä¸ªåºåˆ—é•¿åº¦ä¸º3ï¼Œç¬¬äºŒä¸ªä¸º4ï¼Œé‚£ä¹ˆself-attentionçš„maskä¸º(2, 1, 1, 4)ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸¤ä¸ªå‘é‡è¡¨ç¤ºï¼š

```
1 1 1 0
1 1 1 1
```

å®ƒçš„æ„æ€æ˜¯åœ¨self-attentioné‡Œï¼Œç¬¬ä¸€ä¸ªåºåˆ—çš„ä»»ä¸€æ—¶åˆ»å¯ä»¥attend to å‰3ä¸ªæ—¶åˆ»(å› ä¸ºç¬¬4ä¸ªæ—¶åˆ»æ˜¯paddingçš„)ï¼›è€Œç¬¬äºŒä¸ªåºåˆ—çš„å¯ä»¥attend toæ‰€æœ‰æ—¶åˆ»çš„è¾“å…¥ã€‚è€ŒDecoderçš„src-attentionçš„maskä¸º(2, 1, 4, 4)ï¼Œæˆ‘ä»¬éœ€è¦ç”¨2ä¸ªçŸ©é˜µè¡¨ç¤ºï¼š(ä¸€ä¸ªåºåˆ—å¯¹åº”ä¸€ä¸ªä¸€ç»´src_maskï¼ˆ1Ã—4ï¼‰ï¼Œ  ä¸€ä¸ªåºåˆ—å¯¹åº”ä¸€ä¸ªäºŒç»´çš„tgt_maskï¼ˆ4Ã—4ï¼‰)

```
ç¬¬ä¸€ä¸ªåºåˆ—çš„maskçŸ©é˜µ
1 0 0 0
1 1 0 0
1 1 1 0
1 1 1 0

ç¬¬äºŒä¸ªåºåˆ—çš„maskçŸ©é˜µ
1 0 0 0
1 1 0 0 
1 1 1 0
1 1 1 1
```



æ¥ä¸‹æ¥å¯¹scoreæ±‚softmaxï¼ŒæŠŠå¾—åˆ†å˜æˆæ¦‚ç‡p_attnï¼Œå¦‚æœæœ‰dropoutè¿˜å¯¹p_attnè¿›è¡ŒDropout(è¿™ä¹Ÿæ˜¯åŸå§‹è®ºæ–‡æ²¡æœ‰çš„)ã€‚æœ€åæŠŠp_attnå’Œvalueç›¸ä¹˜ã€‚p_attnæ˜¯(30, 8, 33, 33)ï¼Œvalueæ˜¯(30, 8, 33, 64)ï¼Œæˆ‘ä»¬**åªçœ‹åä¸¤ç»´ï¼Œ(33x33) x (33x64)æœ€ç»ˆå¾—åˆ°33x64ã€‚**



------



æ¥ä¸‹æ¥å°±æ˜¯è¾“å…¥æ€ä¹ˆå˜æˆQ,Kå’ŒVäº†ï¼Œ**å¯¹äºæ¯ä¸€ä¸ªHeadï¼Œéƒ½ä½¿ç”¨ä¸‰ä¸ªçŸ©é˜µWQ,WK,WVæŠŠè¾“å…¥è½¬æ¢æˆQï¼ŒKå’ŒVã€‚**ç„¶å**åˆ†åˆ«ç”¨æ¯ä¸€ä¸ªHeadè¿›è¡ŒSelf-Attentionçš„è®¡ç®—ï¼Œæœ€åæŠŠNä¸ªHeadçš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œæœ€åç”¨ä¸€ä¸ªçŸ©é˜µWOæŠŠè¾“å‡ºå‹ç¼©ä¸€ä¸‹ã€‚**å…·ä½“è®¡ç®—è¿‡ç¨‹ä¸ºï¼š

<img src="https://i.loli.net/2020/08/06/1IbPcFJeK8tsHqN.png" alt="image-20200806023820900" style="zoom: 67%;" />



è¯¦ç»†ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¾“å…¥Qï¼ŒKå’ŒVç»è¿‡å¤šä¸ªçº¿æ€§å˜æ¢åå¾—åˆ°N(8)ç»„Queryï¼ŒKeyå’ŒValueï¼Œç„¶åä½¿ç”¨Self-Attentionè®¡ç®—å¾—åˆ°Nä¸ªå‘é‡ï¼Œç„¶åæ‹¼æ¥èµ·æ¥ï¼Œ**æœ€åä½¿ç”¨ä¸€ä¸ªçº¿æ€§å˜æ¢è¿›è¡Œé™ç»´ã€‚**



<img src="https://i.loli.net/2020/08/08/a2gozSYGn8NOkpH.png" alt="png" style="zoom:67%;" />

```python
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        "Take in model size and number of heads."
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0  # ä¸èƒ½æ•´é™¤å°±æŠ¥é”™
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # # æ‰€æœ‰hä¸ªheadçš„maskéƒ½æ˜¯ç›¸åŒçš„ 
            mask = mask.unsqueeze(1) #åœ¨ç»´åº¦ä¸º1çš„ä½ç½®æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼Œæ•°å­—ä¸º1
        nbatches = query.size(0) #å°±æ˜¯æœ‰å¤šå°‘batchçš„å€¼
        
        # 1) é¦–å…ˆä½¿ç”¨çº¿æ€§å˜æ¢ï¼Œç„¶åæŠŠd_modelåˆ†é…ç»™hä¸ªHeadï¼Œæ¯ä¸ªheadä¸ºd_k=d_model/h 
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
           #.view()è¡¨ç¤ºé‡æ„å¼ é‡çš„ç»´åº¦
         #æ³¨ï¼šå› ä¸ºæ¯ä¸ªLinearå­¦ä¹ åˆ°çš„å‚æ•°æ˜¯ä¸ä¸€æ ·çš„ã€‚æ‰€ä»¥qkvä¸‰ä¸ªä¹Ÿæ˜¯ä¸ä¸€æ ·çš„
            
            
        # 2)ä½¿ç”¨attentionå‡½æ•°è®¡ç®— 
        x, self.attn = attention(query, key, value, mask=mask, 
                                 dropout=self.dropout)
        
        # 3) æŠŠ8ä¸ªheadçš„64ç»´å‘é‡æ‹¼æ¥æˆä¸€ä¸ª512çš„å‘é‡ã€‚ç„¶åå†ä½¿ç”¨ä¸€ä¸ªçº¿æ€§å˜æ¢(512,521)ï¼Œshapeä¸å˜ã€‚ 
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
```



æˆ‘ä»¬å…ˆçœ‹æ„é€ å‡½æ•°ï¼Œè¿™é‡Œ**d_model(512)æ˜¯Multi-Headçš„è¾“å‡ºå¤§å°**ï¼Œå› ä¸ºæœ‰h(8)ä¸ªheadï¼Œå› æ­¤æ¯ä¸ªheadçš„d_k=512/8=64ã€‚æ¥ç€æˆ‘ä»¬æ„é€ 4ä¸ª(d_model ï¼Œ d_model)çš„çŸ©é˜µï¼Œåé¢æˆ‘ä»¬ä¼šçœ‹åˆ°å®ƒçš„ç”¨å¤„ã€‚æœ€åæ˜¯æ„é€ ä¸€ä¸ªDropoutå±‚ã€‚

ç„¶åæˆ‘ä»¬æ¥çœ‹forwardæ–¹æ³•ã€‚**è¾“å…¥çš„maskæ˜¯(batch, 1, time)çš„ï¼Œå› ä¸ºæ¯ä¸ªheadçš„maskéƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥å…ˆç”¨unsqueeze(1)å˜æˆ(batch, 1, 1, time)**ï¼Œmaskæˆ‘ä»¬å‰é¢å·²ç»è¯¦ç»†åˆ†æè¿‡äº†ã€‚

æ¥ä¸‹æ¥æ˜¯**æ ¹æ®è¾“å…¥queryï¼Œkeyå’Œvalueè®¡ç®—å˜æ¢åçš„Multi-Headçš„queryï¼Œkeyå’Œvalue**ã€‚è¿™æ˜¯é€šè¿‡ä¸‹é¢çš„è¯­å¥æ¥å®ç°çš„ï¼š

```python
query, key, value = \
		[l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)	
			for l, x in zip(self.linears, (query, key, value))] # l(x): è°ƒç”¨nn.Linearå‡½æ•°
```

**zip(self.linears, (query, key, value))æ˜¯æŠŠ(self.linears[0],self.linears[1],self.linears[2])å’Œ(query, key, value)æ”¾åˆ°ä¸€èµ·ç„¶åéå†ã€‚æˆ‘ä»¬åªçœ‹ä¸€ä¸ªself.linears[0] (query)ã€‚æ ¹æ®æ„é€ å‡½æ•°çš„å®šä¹‰ï¼Œself.linears[0]æ˜¯ä¸€ä¸ª(512, 512)çš„çŸ©é˜µï¼Œè€Œqueryæ˜¯(batch, time, 512)ï¼Œç›¸ä¹˜ä¹‹åå¾—åˆ°çš„æ–°queryè¿˜æ˜¯512(d_model)ç»´çš„å‘é‡ï¼Œç„¶åç”¨viewæŠŠå®ƒå˜æˆ(batch, time, 8, 64)ã€‚ç„¶åtransponseæˆ(batch, 8,time,64)ï¼Œè¿™æ˜¯attentionå‡½æ•°è¦æ±‚çš„shapeã€‚åˆ†åˆ«å¯¹åº”8ä¸ªHeadï¼Œæ¯ä¸ªHeadçš„Queryéƒ½æ˜¯64ç»´ã€‚**

> 1.ä¸€èˆ¬æ¥è¯´ï¼ŒçŸ©é˜µç›¸ä¹˜ï¼Œ[a,b] x [b,c] = [a,c]
>
> æ‰€ä»¥ä¸åŒç»´åº¦è¦è¿›è¡Œå¤„ç†ï¼Œå¿…é¡»é™ç»´ã€‚ä¾‹å¦‚ A çŸ©é˜µ [a,b,c], B çŸ©é˜µæ˜¯[c,d]
>
> è¿™ä¸ªæ—¶å€™å°±éœ€è¦å°† A çŸ©é˜µçœ‹æˆæ˜¯ [axb, c] ä¸ [c,d] è¿›è¡Œç›¸ä¹˜ï¼Œå¾—åˆ°ç»“æœã€‚
>
> 2. Linearå‡½æ•°l(x)ï¼Œåº”è¯¥å°±æ˜¯ (batch*time,512)**(512,512)

Keyå’ŒValueçš„è¿ç®—å®Œå…¨ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿåˆ†åˆ«å¾—åˆ°8ä¸ªHeadçš„64ç»´çš„Keyå’Œ64ç»´çš„Valueã€‚æ¥ä¸‹æ¥**è°ƒç”¨attentionå‡½æ•°ï¼Œå¾—åˆ°xå’Œself.attnã€‚å…¶ä¸­xçš„shapeæ˜¯(batch, 8, time, 64)ï¼Œè€Œattnæ˜¯(batch, 8, time, time)ã€‚**

**x.transpose(1, 2)æŠŠxå˜æˆ(batch, time, 8, 64)ï¼Œç„¶åæŠŠå®ƒviewæˆ(batch, time, 512)ï¼Œå…¶å®å°±æ˜¯æŠŠæœ€å8ä¸ª64ç»´çš„å‘é‡æ‹¼æ¥æˆ512çš„å‘é‡ã€‚æœ€åä½¿ç”¨self.linears[-1]å¯¹xè¿›è¡Œçº¿æ€§å˜æ¢ï¼Œself.linears[-1]æ˜¯(512, 512)çš„ï¼Œå› æ­¤æœ€ç»ˆçš„è¾“å‡ºè¿˜æ˜¯(batch, time, 512)ã€‚æˆ‘ä»¬æœ€åˆæ„é€ äº†4ä¸ª(512, 512)çš„çŸ©é˜µï¼Œå‰3ä¸ªç”¨äºå¯¹queryï¼Œkeyå’Œvalueè¿›è¡Œå˜æ¢ï¼Œè€Œæœ€åä¸€ä¸ªå¯¹8ä¸ªheadæ‹¼æ¥åçš„å‘é‡å†åšä¸€æ¬¡å˜æ¢ã€‚**



#### A0ttentionåœ¨æ¨¡å‹ä¸­çš„åº”ç”¨

åœ¨Transformeré‡Œï¼Œæœ‰3ä¸ªåœ°æ–¹ç”¨åˆ°äº†MultiHeadedAttentionï¼š

- Encoderçš„Self-Attentionå±‚

  **queryï¼Œkeyå’Œvalueéƒ½æ˜¯ç›¸åŒçš„å€¼**ï¼Œæ¥è‡ªä¸‹å±‚çš„è¾“å…¥ã€‚Maskéƒ½æ˜¯1(å½“ç„¶paddingçš„ä¸ç®—)ã€‚

- Decoderçš„Self-Attentionå±‚

  **queryï¼Œkeyå’Œvalueéƒ½æ˜¯ç›¸åŒçš„å€¼**ï¼Œæ¥è‡ªä¸‹å±‚çš„è¾“å…¥ã€‚ä½†æ˜¯Maskä½¿å¾—å®ƒä¸èƒ½è®¿é—®æœªæ¥çš„è¾“å…¥ã€‚

- Encoder-Decoderçš„æ™®é€šAttention

  **queryæ¥è‡ªä¸‹å±‚çš„è¾“å…¥ï¼Œè€Œkeyå’Œvalueç›¸åŒ**ï¼Œæ˜¯Encoderæœ€åä¸€å±‚çš„è¾“å‡ºï¼Œè€ŒMaskéƒ½æ˜¯1ã€‚

  

### Position-wise å‰é¦ˆç½‘ç»œ

In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. `This consists of two linear transformations with a ReLU activation in between.`

å…¨è¿æ¥å±‚æœ‰ä¸¤ä¸ªçº¿æ€§å˜æ¢ä»¥åŠå®ƒä»¬ä¹‹é—´çš„ReLUæ¿€æ´»ç»„æˆï¼š

<img src="https://i.loli.net/2020/08/08/PU96rciRsWxOCKp.png" alt="image-20200728231445307" style="zoom:50%;" />

å…¨è¿æ¥å±‚çš„è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯d_model(512)ç»´çš„ï¼Œä¸­é—´éšå•å…ƒçš„ä¸ªæ•°æ˜¯d_ff(2048)ç»´

```python
class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))
```

### Embeddings å’Œ Softmax



**è¾“å…¥çš„è¯åºåˆ—éƒ½æ˜¯IDåºåˆ—ï¼Œæˆ‘ä»¬éœ€è¦Embedding**ã€‚æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€éƒ½éœ€è¦Embeddingï¼Œæ­¤å¤–æˆ‘ä»¬éœ€è¦ä¸€ä¸ªçº¿æ€§å˜æ¢æŠŠéšå˜é‡å˜æˆè¾“å‡ºæ¦‚ç‡ï¼Œè¿™å¯ä»¥é€šè¿‡å‰é¢çš„ç±»Generatoræ¥å®ç°ã€‚æˆ‘ä»¬è¿™é‡Œå®ç°Embeddingï¼š

```python
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model) #å°†å­—å…¸vocabå¤§å°æ˜ å°„åˆ°d_modelå¤§å°
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
```

æ³¨æ„çš„å°±æ˜¯forwardå¤„ç†ä½¿ç”¨nn.Embeddingå¯¹è¾“å…¥xè¿›è¡ŒEmbeddingä¹‹å¤–ï¼Œè¿˜é™¤ä»¥äº†sqrt(d_model) ï¼ˆå¼€æ–¹ï¼‰



### ä½ç½®ç¼–ç 

ä½ç½®ç¼–ç çš„å…¬å¼ä¸ºï¼š

 <img src="https://i.loli.net/2020/08/08/WUpXhHsK3S1jCqn.png" alt="image-20200728232133981" style="zoom:50%;" />

<img src="https://i.loli.net/2020/08/08/XOZPy89KVi1xjTh.png" alt="image-20200728232255029" style="zoom:50%;" />

 where `pos` is the position and `i` is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. 

å‡è®¾è¾“å…¥æ˜¯IDåºåˆ—é•¿åº¦ä¸º10ï¼Œ**å¦‚æœè¾“å…¥Embeddingä¹‹åæ˜¯(10, 512)ï¼Œé‚£ä¹ˆä½ç½®ç¼–ç çš„è¾“å‡ºä¹Ÿæ˜¯(10, 512)ã€‚**ä¸Šå¼ä¸­poså°±æ˜¯ä½ç½®(0-9)ï¼Œ512ç»´çš„å¶æ•°ç»´ä½¿ç”¨sinå‡½æ•°ï¼Œè€Œå¥‡æ•°ç»´ä½¿ç”¨coså‡½æ•°ã€‚è¿™ç§ä½ç½®ç¼–ç çš„å¥½å¤„æ˜¯ï¼šPE_pos+kå¯ä»¥è¡¨ç¤ºæˆ PE_posçš„çº¿æ€§å‡½æ•°ï¼Œè¿™æ ·ç½‘ç»œå°±èƒ½å®¹æ˜“çš„å­¦åˆ°ç›¸å¯¹ä½ç½®çš„å…³ç³»ã€‚

```python
plt.figure(figsize=(15, 5))
pe = PositionalEncoding(20, 0)
y = pe.forward(Variable(torch.zeros(1, 100, 20)))
plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())
plt.legend(["dim %d"%p for p in [4,5,6,7]])
```

å›¾æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå‘é‡çš„å¤§å°d_model=20ï¼Œæˆ‘ä»¬è¿™é‡Œç”»å‡ºæ¥ç¬¬4ã€5ã€6å’Œ7ç»´(ä¸‹æ ‡ä»é›¶å¼€å§‹)ç»´çš„å›¾åƒï¼Œæœ€å¤§çš„ä½ç½®æ˜¯100ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä»¬éƒ½æ˜¯æ­£å¼¦(ä½™å¼¦)å‡½æ•°ï¼Œè€Œä¸”å‘¨æœŸè¶Šæ¥è¶Šé•¿ã€‚



![png](https://i.loli.net/2020/08/08/TfDHKvnM3emYysL.png)



å‰é¢æˆ‘ä»¬æåˆ°ä½ç½®ç¼–ç çš„å¥½å¤„æ˜¯PE_pos+kå¯ä»¥è¡¨ç¤ºæˆ P_Eposçš„çº¿æ€§å‡½æ•°ï¼Œæˆ‘ä»¬ä¸‹é¢ç®€å•çš„éªŒè¯ä¸€ä¸‹ã€‚æˆ‘ä»¬ä»¥ç¬¬iç»´ä¸ºä¾‹ï¼Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬æŠŠ<img src="https://i.loli.net/2020/08/06/iEoDOvKzB42N6Xe.png" alt="image-20200806104700979" style="zoom: 67%;" />è®°ä½œWiï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸é‡ã€‚



<img src="https://i.loli.net/2020/08/06/E9h2vXIDK1MAjUg.png" alt="image-20200806104725624" style="zoom:67%;" />

æˆ‘ä»¬å‘ç°PE_pos+k ç¡®å®å¯ä»¥è¡¨ç¤ºæˆ PE_posçš„çº¿æ€§å‡½æ•°ã€‚



In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of `Pdrop=0.1`.

```python
class PositionalEncoding(nn.Module):
    "Implement the PE function."
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        #ä¹‹æ‰€ä»¥ç”¨logå†exp,å¯èƒ½æ˜¯è€ƒè™‘åˆ°æ•°å€¼è¿‡å¤§æº¢å‡ºçš„é—®é¢˜
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], 
                         requires_grad=False)
        return self.dropout(x)
```

ä»£ç å¯ä»¥å‚è€ƒå…¬å¼ï¼Œè°ƒç”¨äº†`Module.register_bufferå‡½æ•°`ã€‚è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯åˆ›å»ºä¸€ä¸ªbufferï¼Œæ¯”å¦‚è¿™é‡ŒæŠŠpeä¿å­˜ä¸‹æ¥ã€‚register_bufferé€šå¸¸ç”¨äºä¿å­˜ä¸€äº›æ¨¡å‹å‚æ•°ä¹‹å¤–çš„å€¼ï¼Œæ¯”å¦‚åœ¨BatchNormä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¿å­˜running_mean(Moving Average)ï¼Œå®ƒä¸æ˜¯æ¨¡å‹çš„å‚æ•°(ä¸ç”¨æ¢¯åº¦ä¸‹é™)ï¼Œä½†æ˜¯æ¨¡å‹ä¼šä¿®æ”¹å®ƒï¼Œè€Œä¸”åœ¨é¢„æµ‹çš„æ—¶å€™ä¹Ÿè¦ä½¿ç”¨å®ƒã€‚è¿™é‡Œä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œpeæ˜¯ä¸€ä¸ªæå‰è®¡ç®—å¥½çš„å¸¸é‡ï¼Œæˆ‘ä»¬åœ¨forwardè¦ç”¨åˆ°å®ƒã€‚æˆ‘ä»¬åœ¨æ„é€ å‡½æ•°é‡Œå¹¶æ²¡æœ‰æŠŠpeä¿å­˜åˆ°selfé‡Œï¼Œä½†æ˜¯åœ¨forwardçš„æ—¶å€™æˆ‘ä»¬å´å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒ(self.pe)ã€‚å¦‚æœæˆ‘ä»¬ä¿å­˜(åºåˆ—åŒ–)æ¨¡å‹åˆ°ç£ç›˜çš„è¯ï¼ŒPyTorchæ¡†æ¶ä¹Ÿä¼šå¸®æˆ‘ä»¬ä¿å­˜bufferé‡Œçš„æ•°æ®åˆ°ç£ç›˜ï¼Œè¿™æ ·ååºåˆ—åŒ–çš„æ—¶å€™èƒ½æ¢å¤å®ƒä»¬



### å®Œæ•´æ¨¡å‹

> Here we `define a function that takes in hyperparameters and produces a full model.`

```python


def make_model(src_vocab, tgt_vocab, N=6, 
               d_model=512, d_ff=2048, h=8, dropout=0.1): #d_ffï¼š feedforwardçš„ç»´åº¦
    "Helper: Construct a model from hyperparameters."
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), 
                             c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))
    
    # This was important from their code. 
    # Initialize parameters with Glorot / fan_avg. éšæœºåˆå§‹åŒ–å‚æ•°ï¼Œè¿™éå¸¸é‡è¦
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform(p)
    return model

# ç¤ºä¾‹: å¯¹modelç®€å•è¾“å…¥å‚æ•°
tmp_model = make_model(10, 10, 2)

```

é¦–å…ˆæŠŠcopy.deepcopyå‘½åä¸ºcï¼Œè¿™æ ·ä½¿ä¸‹é¢çš„ä»£ç ç®€æ´ä¸€ç‚¹ã€‚ç„¶åæ„é€ MultiHeadedAttentionï¼ŒPositionwiseFeedForwardå’ŒPositionalEncodingå¯¹è±¡ã€‚æ¥ç€å°±æ˜¯æ„é€ EncoderDecoderå¯¹è±¡ã€‚å®ƒéœ€è¦5ä¸ªå‚æ•°ï¼šEncoderã€Decoderã€src-embedã€tgt-embedå’ŒGeneratorã€‚

æˆ‘ä»¬å…ˆçœ‹åé¢ä¸‰ä¸ªç®€å•çš„å‚æ•°ï¼ŒGeneratorç›´æ¥æ„é€ å°±è¡Œäº†ï¼Œå®ƒçš„ä½œç”¨æ˜¯æŠŠæ¨¡å‹çš„éšå•å…ƒå˜æˆè¾“å‡ºè¯çš„æ¦‚ç‡ã€‚è€Œsrc-embedæ˜¯ä¸€ä¸ªEmbeddingså±‚å’Œä¸€ä¸ªä½ç½®ç¼–ç å±‚c(position)ï¼Œtgt-embedä¹Ÿæ˜¯ç±»ä¼¼çš„ã€‚

æœ€åæˆ‘ä»¬æ¥çœ‹Decoder(Encoderå’ŒDecoderç±»ä¼¼çš„)ã€‚Decoderç”±Nä¸ªDecoderLayerç»„æˆï¼Œè€ŒDecoderLayeréœ€è¦ä¼ å…¥self-attn, src-attnï¼Œå…¨è¿æ¥å±‚å’ŒDropoutã€‚å› ä¸ºæ‰€æœ‰çš„MultiHeadedAttentionéƒ½æ˜¯ä¸€æ ·çš„ï¼Œå› æ­¤æˆ‘ä»¬ç›´æ¥deepcopyå°±è¡Œï¼›åŒç†æ‰€æœ‰çš„PositionwiseFeedForwardä¹Ÿæ˜¯ä¸€æ ·çš„ç½‘ç»œç»“æœï¼Œæˆ‘ä»¬å¯ä»¥deepcopyè€Œä¸è¦å†æ„é€ ä¸€ä¸ªã€‚



### è®­ç»ƒ

This section describes the training regime for our models.

> We stop for a quick interlude to introduce some of the tools needed to train a standard encoder decoder model. First `we define a batch object that holds the src and target sentences for training, as well as constructing the masks.`

#### Batches å’Œ Masking

`mask çŸ©é˜µæ¥è‡ª batch`

`self.src_mask = (src != pad).unsqueeze(-2)` ä¹Ÿå°±æ˜¯è¯´, æºè¯­è¨€çš„ **mask çŸ©é˜µçš„ç»´åº¦æ˜¯ (batch_size, 1, length)**, é‚£ä¹ˆä¸ºä»€ä¹ˆ `attn_shape = (batch_size, size, size)` å‘¢? å¯ä»¥è¿™ä¹ˆè§£é‡Š, **åœ¨ encoder é˜¶æ®µçš„ Self_Attention é˜¶æ®µ, æ‰€æœ‰çš„ Attention æ˜¯å¯ä»¥åŒæ—¶è¿›è¡Œçš„, æŠŠæ‰€æœ‰çš„ Attention_result ç®—å‡ºæ¥, ç„¶åç”¨åŒä¸€ä¸ª mask vector * Attention_result å°±å¯ä»¥äº†**, ä½†æ˜¯åœ¨ decoder é˜¶æ®µå´ä¸èƒ½è¿™ä¹ˆåš, æˆ‘ä»¬éœ€è¦å…³æ³¨çš„é—®é¢˜æ˜¯:

> æ ¹æ®å·²ç»é¢„æµ‹å‡ºæ¥çš„å•è¯é¢„æµ‹ä¸‹é¢çš„å•è¯, è¿™ä¸€è¿‡ç¨‹**æ˜¯åºåˆ—çš„**,
>
> è€Œæˆ‘ä»¬çš„è®¡ç®—æ˜¯**å¹¶è¡Œ**çš„, æ‰€ä»¥è¿™ä¸€è¿‡ç¨‹ä¸­, å¿…é¡»è¦å¼•å…¥çŸ©é˜µ. ä¹Ÿå°±æ˜¯ä¸Šé¢çš„ subsequent_mask() å‡½æ•°è·å¾—çš„çŸ©é˜µ.

è¿™ä¸ªçŸ©é˜µä¹Ÿå¾ˆå½¢è±¡, åˆ†åˆ«è¡¨ç¤ºå·²ç»é¢„æµ‹çš„å•è¯çš„ä¸ªæ•°ä¸º, 1, 2, 3, 4, 5.

ç„¶åæˆ‘ä»¬å°†ä»¥ä¸Šè¿‡ç¨‹åè¿‡æ¥è¿‡ä¸€ç¯‡, å°±å¾ˆæ˜æ˜¾äº†, åœ¨ batché˜¶æ®µè·å¾— mask çŸ©é˜µ, ç„¶åå’Œ batch ä¸€èµ·è®­ç»ƒ, åœ¨ encoder ä¸ deocder é˜¶æ®µå®ç° mask æœºåˆ¶.



> - maskåœ¨Batchä¸­å®šä¹‰ï¼Œsrc_mask.size (30,1,10) ,  trg_mask.size(30,10,10)
>
> - ç„¶ååœ¨MultiHeadedAttentionä¸­`mask = mask.unsqueeze(1)`åˆæ‰©ç»´äº†ï¼Œ
>
>   å…¶ä¸­src_mask.size(30,1,1,10) ,trg_mask.size(30,1,10,10)
>
> - src_mask.sizeæ»¡è¶³attentionä¸­çš„ç»´åº¦ï¼Œæ‰€ä»¥å¯ä»¥å¯¹scoreè¿›è¡Œmask
>
>    src_maskè¿˜åœ¨è§£ç å™¨çš„ç¬¬1å­å±‚ç”¨åˆ°ï¼Œç›¸åŒçš„åŸç†
>
> - trg_maskåœ¨è§£ç å™¨çš„ç¬¬0å­å±‚ç”¨åˆ°ï¼Œæ»¡è¶³è¦æ±‚

```python
class Batch: #å®šä¹‰æ¯ä¸€ä¸ªbatchä¸­çš„srcã€tgtã€mask
    #trg = tgt: çœŸå®çš„æ ‡ç­¾åºåˆ—  out ï¼š é¢„æµ‹çš„å•è¯  
    "Object for holding a batch of data with mask during training."
    def __init__(self, src, trg=None, pad=0): 
        self.src = src
        self.src_mask = (src != pad).unsqueeze(-2) #æ‰©å……ç»´åº¦ å€’æ•°ç¬¬äºŒç»´å¢åŠ å€¼ä¸º1 size=(30,1,10)
        #å¹¶ä¸”éé›¶å€¼å…¨éƒ¨èµ‹å€¼ä¸º1
        
         # åœ¨é¢„æµ‹çš„æ—¶å€™æ˜¯æ²¡æœ‰ tgt çš„,æ­¤æ—¶ä¸º None æ­¤æ—¶trgæ˜¯tgtçš„å½¢å‚
        if trg is not None:
            self.trg = trg[:, :-1] #trg.size(30,9) ï¼Œåœ¨é¢„æµ‹ä¸­ï¼Œä¼šæå‰è¾“å…¥èµ·å§‹ç¬¦åˆ°ysä¸­
            """
              trg.size(30,9) è¿™é‡Œå»æ‰çš„æœ€åä¸€ä¸ªå•è¯, ä¸æ˜¯çœŸæ­£çš„å•è¯, è€Œæ˜¯æ ‡å¿— '<eos>' , 						è¾“å…¥ä¸è¾“å‡ºéƒ½è¿˜æœ‰ä¸€ä¸ª '<sos>' åœ¨å¥å­çš„å¼€å¤´,  æ˜¯decoderçš„è¾“å…¥ï¼Œ
            éœ€è¦è¿›è¡Œmaskï¼Œä½¿å¾—Self-Attentionä¸èƒ½è®¿é—®æœªæ¥çš„è¾“å…¥ã€‚æœ€åä¸€ä¸ªè¯ä¸éœ€è¦ç”¨åˆ°trg
            """
    	    self.trg_y = trg[:, 1:] # trg_y.size(30,9) 
            #trg_y: æœ€åçš„ç»“æœã€‚ç”¨äºlossä¸­çš„æ¯”è¾ƒã€‚ å»æ‰å¼€å¤´çš„'<sos>'ï¼Œæ˜¯decoderçš„è¾“å‡º
            self.trg_mask = \
                self.make_std_mask(self.trg, pad)
            self.ntokens = (self.trg_y != pad).data.sum() #ä¸ä¸º0çš„æ€»æ•° 30*9 = 270
    
    @staticmethod
    def make_std_mask(tgt, pad): #tgt_mask.size(30,9,9)ï¼Œæ¯ä¸€ä¸ªåºåˆ—éƒ½æ˜¯ä¸€ä¸ª9*9çš„çŸ©é˜µ
        "Create a mask to hide padding and future words."
        #"åˆ›å»ºMaskï¼Œä½¿å¾—æˆ‘ä»¬ä¸èƒ½attend toæœªæ¥çš„è¯"
        tgt_mask = (tgt != pad).unsqueeze(-2)
        tgt_mask = tgt_mask & Variable(
            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
        return tgt_mask
```

Batchæ„é€ å‡½æ•°çš„è¾“å…¥æ˜¯srcå’Œtrgï¼Œåè€…å¯ä»¥ä¸ºNoneï¼Œå› ä¸ºå†é¢„æµ‹çš„æ—¶å€™æ˜¯æ²¡æœ‰tgtçš„ã€‚

æˆ‘ä»¬ç”¨ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜Batchçš„ä»£ç ï¼Œè¿™æ˜¯è®­ç»ƒé˜¶æ®µçš„ä¸€ä¸ªBatchï¼Œ**srcæ˜¯(48, 20)**ï¼Œ48æ˜¯batchå¤§å°ï¼Œè€Œ20æ˜¯æœ€é•¿çš„å¥å­é•¿åº¦ï¼Œå…¶å®ƒçš„ä¸å¤Ÿé•¿çš„éƒ½paddingæˆ20äº†ã€‚è€Œ**trgæ˜¯(48, 25)**ï¼Œè¡¨ç¤ºç¿»è¯‘åçš„æœ€é•¿å¥å­æ˜¯25ä¸ªè¯ï¼Œä¸è¶³çš„ä¹Ÿpaddingè¿‡äº†ã€‚

æˆ‘ä»¬é¦–å…ˆçœ‹src_maskæ€ä¹ˆå¾—åˆ°ï¼Œ(src != pad)æŠŠsrcä¸­å¤§äº0çš„æ—¶åˆ»ç½®ä¸º1ï¼Œè¿™æ ·è¡¨ç¤ºå®ƒå¯ä»¥attend toçš„èŒƒå›´ã€‚ç„¶åunsqueeze(-2)æŠŠsrc_maskå˜æˆ(48/batch, 1, 20/time)ã€‚å®ƒçš„ç”¨æ³•å‚è€ƒå‰é¢çš„attentionå‡½æ•°ã€‚

å¯¹äºè®­ç»ƒæ¥è¯´(Teaching Forcingæ¨¡å¼)ï¼ŒDecoderæœ‰ä¸€ä¸ªè¾“å…¥å’Œä¸€ä¸ªè¾“å‡ºã€‚**æ¯”å¦‚å¥å­â€<sos> it is a good day <eos>â€ï¼Œè¾“å…¥ä¼šå˜æˆâ€<sos> it is a good dayâ€ï¼Œè€Œè¾“å‡ºä¸ºâ€it is a good day <eos>â€ã€‚å¯¹åº”åˆ°ä»£ç é‡Œï¼Œself.trgå°±æ˜¯è¾“å…¥ï¼Œè€Œself.trg_yå°±æ˜¯è¾“å‡ºã€‚**æ¥ç€å¯¹è¾“å…¥self.trgè¿›è¡Œmaskï¼Œä½¿å¾—Self-Attentionä¸èƒ½è®¿é—®æœªæ¥çš„è¾“å…¥ã€‚è¿™æ˜¯é€šè¿‡make_std_maskå‡½æ•°å®ç°çš„ï¼Œè¿™ä¸ªå‡½æ•°ä¼šè°ƒç”¨æˆ‘ä»¬ä¹‹å‰è¯¦ç»†ä»‹ç»è¿‡çš„subsequent_maskå‡½æ•°ã€‚æœ€ç»ˆå¾—åˆ°çš„**trg_maskçš„shapeæ˜¯(48/batch, 24, 24)**ï¼Œè¡¨ç¤º24ä¸ªæ—¶åˆ»çš„MaskçŸ©é˜µï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹è§’çº¿ä»¥åŠä¹‹ä¸‹éƒ½æ˜¯1çš„çŸ©é˜µï¼Œå‰é¢å·²ç»ä»‹ç»è¿‡äº†ã€‚

æ³¨æ„**src_maskçš„shapeæ˜¯(batch, 1, time)**ï¼Œè€Œ**trg_maskæ˜¯(batch, time, time)**ã€‚å› ä¸ºsrc_maskçš„æ¯ä¸€ä¸ªæ—¶åˆ»éƒ½èƒ½attend toæ‰€æœ‰æ—¶åˆ»(paddingçš„é™¤å¤–)ï¼Œä¸€æ¬¡åªéœ€è¦ä¸€ä¸ªå‘é‡å°±è¡Œäº†ï¼Œè€Œtrg_maskéœ€è¦ä¸€ä¸ªçŸ©é˜µã€‚



#### Training Loop

```python
def run_epoch(data_iter, model, loss_compute): #è¿”å›total_loss / total_tokens ã€‚æ˜¯ä¸€ä¸ªæ•°å€¼ï¼ŒæŸå¤±è®¡ç®—
    #éå†ä¸€ä¸ªepochçš„æ•°æ®
    "Standard Training and Logging Function"
    start = time.time() #å¼€å§‹æ—¶é—´ï¼Œè®¡ç®—ç”¨æ—¶
    total_tokens = 0 
    total_loss = 0 
    tokens = 0 
    for i, batch in enumerate(data_iter): #æ¯ä¸€æ­¥data_iterï¼ˆgen_dataï¼‰ï¼Œå®ä¾‹åŒ–batchæ•°æ®ç”¨äºå­¦ä¹ .è¿›è¡Œ20æ¬¡
        #gen_dataè¿”å›çš„æ˜¯20ä¸ªBatchï¼Œé€šè¿‡enumerateå®ä¾‹åŒ–20ä¸ªbatch 
        out = model.forward(batch.src, batch.trg, 
                            batch.src_mask, batch.trg_mask) #è°ƒç”¨EncoderDecoderçš„å®ä¾‹åŒ–modelï¼Œè§£ç å™¨ä½œä¸ºè¾“å‡º
        loss = loss_compute(out, batch.trg_y, batch.ntokens) #è®¡ç®—å‡ºä¸€ä¸ªbatchä¸­çš„lossã€‚ trg_yæ˜¯æ ‡å‡†å€¼ã€‚ntokensä½œä¸ºnorm
        total_loss += loss #losså åŠ ã€‚è¿›è¡Œ20æ¬¡
        total_tokens += batch.ntokens 
        tokens += batch.ntokens
        if i % 50 == 1: #iä»0å¼€å§‹çš„ï¼Œå½“i=1çš„æ—¶å€™ï¼Œè¿›è¡Œäº†ä¸€æ¬¡batchï¼Œæ‰€ä»¥è¿™é‡Œè®¡ç®—çš„å°±æ˜¯ä¸€æ¬¡batchæ‰€ç”¨çš„æ—¶é—´ã€‚è€Œè¦è¿›è¡Œ20æ¬¡ã€‚  50æ˜¯éšæœºè®¾ç½®
            elapsed = time.time() - start #è®¡ç®—ä¸€å…±ç”¨æ—¶
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" % 
                    (i, loss / batch.ntokens, tokens / elapsed)) #æ‰€æœ‰batchä¸­çš„losså’Œntoken,å³ä¸€ä¸ªepochä¸­
            start = time.time() # é‡ç½®æ—¶é—´
            tokens = 0
    return total_loss / total_tokens
```

å®ƒéå†ä¸€ä¸ªepochçš„æ•°æ®ï¼Œç„¶åè°ƒç”¨forwardï¼Œæ¥ç€ç”¨loss_computeå‡½æ•°è®¡ç®—æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°å¹¶ä¸”è¿”å›lossã€‚è¿™é‡Œçš„loss_computeæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒçš„è¾“å…¥æ˜¯æ¨¡å‹çš„é¢„æµ‹outï¼ŒçœŸå®çš„æ ‡ç­¾åºåˆ—batch.trg_yå’Œbatchçš„è¯ä¸ªæ•°ã€‚å®é™…çš„å®ç°æ˜¯MultiGPULossComputeç±»ï¼Œè¿™æ˜¯ä¸€ä¸ªcallableã€‚æœ¬æ¥è®¡ç®—æŸå¤±å’Œæ›´æ–°å‚æ•°æ¯”è¾ƒç®€å•ï¼Œä½†æ˜¯è¿™é‡Œä¸ºäº†å®ç°å¤šGPUçš„è®­ç»ƒï¼Œè¿™ä¸ªç±»å°±æ¯”è¾ƒå¤æ‚äº†ã€‚



#### Training Data å’Œ Batching

We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English- French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.

Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.

> We will use torch text for batching. This is discussed in more detail below. Here we create batches in a torchtext function that ensures our batch size padded to the maximum batchsize does not surpass a threshold (25000 if we have 8 gpus).

```python
global max_src_in_batch, max_tgt_in_batch
def batch_size_fn(new, count, sofar):
    "Keep augmenting batch and calculate total number of tokens + padding."
    global max_src_in_batch, max_tgt_in_batch
    if count == 1:
        max_src_in_batch = 0
        max_tgt_in_batch = 0
    max_src_in_batch = max(max_src_in_batch,  len(new.src))
    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)
    src_elements = count * max_src_in_batch
    tgt_elements = count * max_tgt_in_batch
    return max(src_elements, tgt_elements)
```

#### ç¡¬ä»¶ å’Œ è®­ç»ƒè¿›åº¦

We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).

#### Optimizer

We used the `Adam optimizer` [(cite)](https://arxiv.org/abs/1412.6980) with Î²1=0.9Î²1=0.9, Î²2=0.98Î²2=0.98 and Ïµ=10âˆ’9Ïµ=10âˆ’9. We varied the learning rate over the course of training, according to the formula: lrate=dâˆ’0.5modelâ‹…min(step_numâˆ’0.5,step_numâ‹…warmup_stepsâˆ’1.5)lrate=dmodelâˆ’0.5â‹…min(step_numâˆ’0.5,step_numâ‹…warmup_stepsâˆ’1.5) This corresponds to increasing the learning rate linearly for the first warmupstepswarmupsteps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmupsteps=4000warmupsteps=4000.

> Note: This part is very important. Need to train with this setup of the model.

```python
class NoamOpt:
    "Optim wrapper that implements rate."
    def __init__(self, model_size, factor, warmup, optimizer):
        self.optimizer = optimizer
        self._step = 0
        self.warmup = warmup
        self.factor = factor
        self.model_size = model_size
        self._rate = 0
        
    def step(self):
        "Update parameters and rate"
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()
        
    def rate(self, step = None):
        "Implement `lrate` above"
        if step is None:
            step = self._step
        return self.factor * \
            (self.model_size ** (-0.5) *
            min(step ** (-0.5), step * self.warmup ** (-1.5)))
        
def get_std_opt(model):
    return NoamOpt(model.src_embed[0].d_model, 2, 4000,
            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))
```

> Example of the curves of this model for different model sizes and for optimization hyperparameters.

```python
# Three settings of the lrate hyperparameters.
opts = [NoamOpt(512, 1, 4000, None), 
        NoamOpt(512, 1, 8000, None),
        NoamOpt(256, 1, 4000, None)]
plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])
plt.legend(["512:4000", "512:8000", "256:4000"])
None
```

![png](http://nlp.seas.harvard.edu/images/the-annotated-transformer_69_0.png)

#### Regularization

##### Label Smoothing

During training, we employed label smoothing of value Ïµls=0.1Ïµls=0.1 [(cite)](https://arxiv.org/abs/1512.00567). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.

> We implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has `confidence` of the correct word and the rest of the `smoothing` mass distributed throughout the vocabulary.

```python
class LabelSmoothing(nn.Module):
    "Implement label smoothing."
    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(size_average=False)  #KLæ•£åº¦
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None
        
    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, Variable(true_dist, requires_grad=False))
```

> Here we can see an example of how the mass is distributed to the words based on confidence.

```python
# Example of label smoothing.
crit = LabelSmoothing(5, 0, 0.4)
predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],
                             [0, 0.2, 0.7, 0.1, 0], 
                             [0, 0.2, 0.7, 0.1, 0]])
v = crit(Variable(predict.log()), 
         Variable(torch.LongTensor([2, 1, 0])))

# Show the target distributions expected by the system.
plt.imshow(crit.true_dist)
None
```

![png](http://nlp.seas.harvard.edu/images/the-annotated-transformer_74_0.png)

> Label smoothing actually starts to penalize the model if it gets very confident about a given choice.

```python
crit = LabelSmoothing(5, 0, 0.1)
def loss(x):
    d = x + 3 * 1
    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],
                                 ])
    #print(predict)
    return crit(Variable(predict.log()),
                 Variable(torch.LongTensor([1]))).data[0]
plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])
None
```

![png](http://nlp.seas.harvard.edu/images/the-annotated-transformer_76_0.png)

### **æ€»ç»“**

transformeræ¨¡å‹ä¸»è¦åˆ†ä¸ºä¸¤å¤§éƒ¨åˆ†, åˆ†åˆ«æ˜¯ç¼–ç å™¨å’Œè§£ç å™¨, ç¼–ç å™¨è´Ÿè´£æŠŠè‡ªç„¶è¯­è¨€åºåˆ—æ˜ å°„æˆä¸ºéšè—å±‚(ä¸‹å›¾ä¸­ç¬¬2æ­¥ç”¨ä¹å®«æ ¼æ¯”å–»çš„éƒ¨åˆ†), å«æœ‰è‡ªç„¶è¯­è¨€åºåˆ—çš„æ•°å­¦è¡¨è¾¾. ç„¶åè§£ç å™¨æŠŠéšè—å±‚å†æ˜ å°„ä¸ºè‡ªç„¶è¯­è¨€åºåˆ—, ä»è€Œä½¿æˆ‘ä»¬å¯ä»¥è§£å†³å„ç§é—®é¢˜, å¦‚æƒ…æ„Ÿåˆ†ç±», å‘½åå®ä½“è¯†åˆ«, è¯­ä¹‰å…³ç³»æŠ½å–, æ‘˜è¦ç”Ÿæˆ, æœº å™¨ç¿»è¯‘ç­‰ç­‰, ä¸‹é¢æˆ‘ä»¬ç®€å•è¯´ä¸€ä¸‹ä¸‹å›¾çš„æ¯ä¸€æ­¥éƒ½åšäº†ä»€ä¹ˆ:

> 1.è¾“å…¥è‡ªç„¶è¯­è¨€åºåˆ—åˆ°ç¼–ç å™¨: Why do we work?(ä¸ºä»€ä¹ˆè¦å·¥ä½œ); 
>
> 2.ç¼–ç å™¨è¾“å‡ºçš„éšè—å±‚, å†è¾“å…¥åˆ°è§£ç å™¨; 
>
> 3.è¾“å…¥<ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡><start>(èµ·å§‹)ç¬¦å·åˆ°è§£ç å™¨; 
>
> 4.å¾—åˆ°ç¬¬ä¸€ä¸ªå­—"ä¸º"; 
>
> 5.å°†å¾—åˆ°çš„ç¬¬ä¸€ä¸ªå­—"ä¸º"è½ä¸‹æ¥å†è¾“å…¥åˆ°è§£ç å™¨; 
>
> 6.å¾—åˆ°ç¬¬äºŒä¸ªå­—"ä»€"; 
>
> 7.å°†å¾—åˆ°çš„ç¬¬äºŒå­—å†è½ä¸‹æ¥, ç›´åˆ°è§£ç å™¨è¾“å‡º<ğ‘’ğ‘›ğ‘‘><end>(ç»ˆæ­¢ç¬¦), å³åºåˆ—ç”Ÿæˆå®Œæˆ.



<img src="https://i.loli.net/2020/08/06/1pea3WSThisHBql.png" alt="image-20200806233205808" style="zoom:67%;" />







![transformer](https://i.loli.net/2020/08/07/ZGa1snNULJtjFWS.png)



åŸå§‹dataæ•°æ®æ˜¯ï¼š(30,10)

src: (30,10)  trg:(30,10) 

åœ¨encoderä¸­ï¼Œ

embeddingï¼š å‚æ•°xå°±æ˜¯ src ï¼ˆ30,10ï¼‰ ç»è¿‡å¤„ç†ä¹‹åï¼Œ x:ï¼ˆ30,10,512ï¼‰ -> å³è¾“å…¥ç»™encoderçš„xï¼š(30,10,512)

ç»è¿‡encoderå„ä¸ªå±‚å¤„ç†ä¹‹åï¼Œè¾“å‡ºçš„ï¼ˆ30ï¼Œ10,512ï¼‰  memoryæ˜¯encoderçš„è¾“å‡ºï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆmemoryï¼šï¼ˆ1,10,512ï¼‰ ??? å› ä¸ºåœ¨é¢„æµ‹æ—¶ ï¼Œsrcæ˜¯ï¼ˆ1,10ï¼‰ï¼Œä¸æ˜¯ï¼ˆ30,10ï¼‰æ‰€ä»¥memoryæ˜¯ï¼ˆ1,10,512ï¼‰



decoderä¸­ï¼šè¾“å…¥æ¥è‡ª memory å’Œ trg_emd

embedding ï¼š å‚æ•°xæ˜¯trgï¼ˆ30,9ï¼‰ï¼Œç»è¿‡å¤„ç†ä¹‹åï¼Œxï¼šï¼ˆ30,9,512)   

ç»è¿‡decoderå„ä¸ªå±‚å¤„ç†ä¹‹åï¼Œè¾“å‡ºçš„ï¼ˆ30ï¼Œ9 , 512ï¼‰  



å†ç»è¿‡generatorå±‚ä¹‹åï¼Œxï¼šï¼ˆ30,9,11ï¼‰ 



åœ¨é¢„æµ‹çš„æ—¶å€™æ˜¯ï¼ˆ1ï¼Œ1,512ï¼‰ï¼Œä¸æ˜¯ï¼ˆ1,9,512ï¼‰ï¼Œåœ¨é¢„æµ‹å®Œgeneratorä¹‹åï¼Œï¼ˆ1,11ï¼‰ï¼Œé€‰ä¸€ä¸ªæœ€å¤§çš„ã€‚

å› ä¸ºæ˜¯ä¸€ä¸ªæ•°å­—ä¸€ä¸ªæ•°å­—é¢„æµ‹è¾“å‡ºçš„ï¼Œæ‰€ä»¥æ˜¯1ï¼Œä¸æ˜¯9



 







### ç¬¬ä¸€ä¸ªä¾‹å­

> We can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols.

#### Synthetic Data

```python
def data_gen(V, batch, nbatches): # batch=30:ä¸€æ¬¡è¾“å…¥å¤šå°‘ï¼Œ nbatch=20ï¼šè¾“å…¥å¤šå°‘æ¬¡
    "Generate random data for a src-tgt copy task."
    for i in range(nbatches): #ä¸€å…±å¾ªç¯nbatchesä¸ªï¼Œåœ¨æ¯ä¸€ä¸ªæ˜¯ä¸€ä¸ªbatch
		#from_numpy ï¼š å°†numpyæ•°æ®è½¬æ¢ä¸ºtensor
		#æ³¨ï¼šç”Ÿæˆè¿”å›çš„tensorä¼šå’Œndarryå…±äº«æ•°æ®ï¼Œä»»ä½•å¯¹tensorçš„æ“ä½œéƒ½ä¼šå½±å“åˆ°ndarry
        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10))) #1æ˜¯äº§ç”Ÿçš„æœ€å°å€¼ï¼ŒV=11æ˜¯æœ€å¤§å€¼ï¼Œsizeæ˜¯å½¢çŠ¶ï¼ˆbatchï¼Œ10ï¼‰ã€‚ç”Ÿæˆï¼ˆbatchï¼Œ10ï¼‰çš„çŸ©é˜µï¼ŒçŸ©é˜µçš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯1~V-1ä¹‹é—´  ï¼ˆå–ä¸åˆ°Vï¼‰
        data[:, 0] = 1 #å°†ç¬¬0åˆ—çš„å€¼èµ‹å€¼ä¸º1
        # Variable å°±æ˜¯ä¸€ä¸ªå­˜æ”¾å€¼ï¼Œ é‡Œé¢çš„å€¼ä¼šä¸åœçš„å˜åŒ–.  å­˜æ”¾çš„æ˜¯Torch çš„ Tensor . å¦‚æœç”¨ä¸€ä¸ª Variable è¿›è¡Œè®¡ç®—, é‚£è¿”å›çš„ä¹Ÿæ˜¯ä¸€ä¸ªåŒç±»å‹çš„ Variable.  
        #requires_gradï¼š æ˜¯å¦å‚ä¸è¯¯å·®åå‘ä¼ æ’­, è¦ä¸è¦è®¡ç®—æ¢¯åº¦
        src = Variable(data, requires_grad=False) #size(batch,10) å’Œdataçš„å€¼å®Œå…¨ä¸€æ ·
        tgt = Variable(data, requires_grad=False)
        yield Batch(src, tgt, 0)#yieldå°±æ˜¯returnä¸€ä¸ªå€¼ï¼Œå¹¶ä¸”è®°ä½è¿™ä¸ªè¿”å›çš„ä½ç½®ï¼Œä¸‹æ¬¡è¿­ä»£å°±ä»è¿™ä¸ªä½ç½®å(ä¸‹ä¸€è¡Œ)å¼€å§‹
        #batchè¿”å›çš„æ˜¯trg_mask 
```

#### Loss Computation

```python
class SimpleLossCompute: #lossè®¡ç®—ä»¥åŠæ›´æ–°ã€‚è°ƒç”¨LabelSmoothingï¼Œä½¿ç”¨KLæ•£åº¦
    "A simple loss compute and train function."
    def __init__(self, generator, criterion, opt=None):
        self.generator = generator #è§£ç å™¨åçš„ç”Ÿæˆå‡½æ•°
        self.criterion = criterion # LabelSmoothingï¼ˆè®¡ç®—loss KLDivLoss KLæ•£åº¦ï¼‰çš„å®ä¾‹åŒ–
        self.opt = opt # NoamOptï¼ˆä¼˜åŒ–ï¼‰çš„å®ä¾‹åŒ–
        
    def __call__(self, x, y, norm):
        x = self.generator(x) #è§£ç å™¨çš„è¾“å‡º
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), 
                              y.contiguous().view(-1)) / norm  #è®¡ç®—loss
        loss.backward() #å°†lossåå‘ä¼ æ’­ã€‚lossæ˜¯æ ‡é‡ï¼Œæ ¹æ®é“¾å¼æ³•åˆ™è‡ªåŠ¨è®¡ç®—å‡ºå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼
        if self.opt is not None: #å­˜åœ¨ä¼˜åŒ–
            self.opt.step() #è°ƒç”¨optçš„stepå‡½æ•°ã€‚ adamä¼˜åŒ–ï¼Œï¼Œæ›´æ–°å‚æ•°
            self.opt.optimizer.zero_grad() #æŠŠæ¢¯åº¦ç½®é›¶ï¼Œä¹Ÿå°±æ˜¯æŠŠlosså…³äºweightçš„å¯¼æ•°å˜æˆ0.
        return loss.data[0] * norm
```

#### Greedy Decoding

```python
# Train the simple copy task.
V = 11
criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0) #LabelSmoothingæ˜¯KLæ•£åº¦å®ç°çš„
model = make_model(V, V, N=2) #src_vocab=11, tgt_vocab=11ï¼Œè¦†ç›–N=2
# å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°ä¼˜åŒ–ï¼Œä½¿ç”¨Adamä¼˜åŒ–
model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,
        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))

#model.eval()ï¼Œpytorchä¼šè‡ªåŠ¨æŠŠBNå’ŒDropOutå›ºå®šä½ï¼Œä¸ä¼šå–å¹³å‡ï¼Œè€Œæ˜¯ç”¨è®­ç»ƒå¥½çš„å€¼ã€‚
#model.train() è®©modelå˜æˆè®­ç»ƒæ¨¡å¼ï¼Œæ­¤æ—¶dropoutå’Œbatch normalizationçš„æ“ä½œåœ¨è®­ç»ƒèµ·åˆ°é˜²æ­¢ç½‘ç»œè¿‡æ‹Ÿåˆçš„é—®é¢˜

for epoch in range(10): #ä¸€å…±10å¤§ä»½ï¼Œ model.train()æ‰“å°1è¡Œï¼Œmodel.eval()æ‰“å°1è¡Œ
    model.train()
    #è°ƒç”¨run_epoch(data_iter, model, loss_compute)å‡½æ•°
    #è¿”å›total_loss / total_tokens ã€‚è¿”å›å€¼å¯ä»¥æ²¡æœ‰æ¥æ”¶ï¼Œä¸ä¼šæŠ¥é”™
    run_epoch(data_gen(V, 30, 20), model, 
              SimpleLossCompute(model.generator, criterion, model_opt))
    model.eval()
    #printæ¥æ”¶run_epochçš„è¿”å›å€¼ åœ¨è¾“å‡ºçš„ç¬¬ä¸‰è¡Œ
    print(run_epoch(data_gen(V, 30, 5), model, 
                    SimpleLossCompute(model.generator, criterion, None)))
Epoch Step: 1 Loss: 3.023465 Tokens per Sec: 403.074173
Epoch Step: 1 Loss: 1.920030 Tokens per Sec: 641.689380
1.9274832487106324
Epoch Step: 1 Loss: 1.940011 Tokens per Sec: 432.003378
Epoch Step: 1 Loss: 1.699767 Tokens per Sec: 641.979665
1.657595729827881
Epoch Step: 1 Loss: 1.860276 Tokens per Sec: 433.320240
Epoch Step: 1 Loss: 1.546011 Tokens per Sec: 640.537198
1.4888023376464843
Epoch Step: 1 Loss: 1.682198 Tokens per Sec: 432.092305
Epoch Step: 1 Loss: 1.313169 Tokens per Sec: 639.441857
1.3485562801361084
Epoch Step: 1 Loss: 1.278768 Tokens per Sec: 433.568756
Epoch Step: 1 Loss: 1.062384 Tokens per Sec: 642.542067
0.9853351473808288
Epoch Step: 1 Loss: 1.269471 Tokens per Sec: 433.388727
Epoch Step: 1 Loss: 0.590709 Tokens per Sec: 642.862135
0.5686767101287842
Epoch Step: 1 Loss: 0.997076 Tokens per Sec: 433.009746
Epoch Step: 1 Loss: 0.343118 Tokens per Sec: 642.288427
0.34273059368133546
Epoch Step: 1 Loss: 0.459483 Tokens per Sec: 434.594030
Epoch Step: 1 Loss: 0.290385 Tokens per Sec: 642.519464
0.2612409472465515
Epoch Step: 1 Loss: 1.031042 Tokens per Sec: 434.557008
Epoch Step: 1 Loss: 0.437069 Tokens per Sec: 643.630322
0.4323212027549744
Epoch Step: 1 Loss: 0.617165 Tokens per Sec: 436.652626
Epoch Step: 1 Loss: 0.258793 Tokens per Sec: 644.372296
0.27331129014492034
```

> This code predicts a translation using greedy decoding for simplicity.

```python
#é¢„æµ‹è¿‡ç¨‹
#é¢„æµ‹çš„æ—¶å€™æ²¡æœ‰ç”¨tgtï¼ˆæ ‡å‡†å€¼ï¼‰ï¼Œè€Œæ˜¯æ¯æ¬¡è§£ç å™¨çš„è¾“å…¥éƒ½æ˜¯ysï¼Œæ˜¯é¢„æµ‹çš„å€¼
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    memory = model.encode(src, src_mask) #memoryæ˜¯ç¼–ç å™¨çš„è¾“å‡º ã€‚æ˜¯ä¸€ä¸ªçŸ©é˜µ
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data) #å¡«å……è¾“å‡ºå¼€å§‹ç¬¦ï¼Œå’Œsrcçš„ç±»å‹ä¸€æ ·ã€‚å¯¹é¢„æµ‹çš„å¥å­è¿›è¡Œåˆå§‹åŒ– ys =1 ï¼ˆ1,1ï¼‰
    for i in range(max_len-1): #0~8 å¯¹æ¯ä¸€ä¸ªè¯éƒ½è¿›è¡Œé¢„æµ‹
        out = model.decode(memory, src_mask, 
                           Variable(ys), 
                           Variable(subsequent_mask(ys.size(1))
                                    .type_as(src.data)))
         # ys çš„ç»´åº¦æ˜¯ batch_size * times ï¼ˆå›ºå®šçš„ï¼‰,   æ‰€ä»¥target_mask çŸ©é˜µå¿…é¡»æ˜¯ys.size(1),æ‰€ä»¥æ˜¯ times * times
        # æ ¹æ® decoder çš„è®­ç»ƒæ­¥éª¤, è¿™é‡Œçš„ out è¾“å‡ºå°±åº”è¯¥æ˜¯ batch_size * (times+1) çš„çŸ©é˜µ
        
        prob = model.generator(out[:, -1]) #generatorè¿”å›çš„æ˜¯softmax
          # out[:, -1] è¿™é‡Œæ˜¯æœ€æ–°çš„ä¸€ä¸ªå•è¯çš„ embedding å‘é‡
        # generator å°±æ˜¯äº§ç”Ÿæœ€åçš„ vocabulary çš„æ¦‚ç‡, æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚
        
        _, next_word = torch.max(prob, dim = 1) # torch.max:æŒ‰ç»´åº¦dim è¿”å›æœ€å¤§å€¼ï¼Œå¹¶ä¸”ä¼šè¿”å›ç´¢å¼•ã€‚next_dataæ¥æ”¶											#ç´¢å¼•
        next_word = next_word.data[0]
        # å°†å¥å­æ‹¼æ¥èµ·æ¥  .type_as: å°†tensorå¼ºåˆ¶è½¬æ¢ä¸ºsrc.data æ ¼å¼çš„
        ys = torch.cat([ys, 
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    return ys

model.eval()
src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )
src_mask = Variable(torch.ones(1, 1, 10) )
print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))
    1     2     3     4     5     6     7     8     9    10
[torch.LongTensor of size 1x10]
```

### çœŸå®ä¾‹å­

> Now we consider a real-world example using the IWSLT German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast.

```python
#!pip install torchtext spacy
#!python -m spacy download en
#!python -m spacy download de
```

#### Data Loading

> We will load the dataset using torchtext and spacy for tokenization.
>
> ç”¨torchtextæ¥åŠ è½½æ•°æ®é›† ï¼Œ ç”¨spacyæ¥åˆ†è¯



<img src="https://i.loli.net/2020/08/07/teSG1hufEjF4Zkv.png" alt="image-20200807001353729" style="zoom: 67%;" />





torchtextç»„ä»¶æµç¨‹ï¼š

> - å®šä¹‰Fieldï¼šå£°æ˜å¦‚ä½•å¤„ç†æ•°æ®ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹æ•°æ®é¢„å¤„ç†çš„é…ç½®ä¿¡æ¯ï¼Œæ¯”å¦‚æŒ‡å®šåˆ†è¯æ–¹æ³•ï¼Œæ˜¯å¦è½¬æˆå°å†™ï¼Œèµ·å§‹å­—ç¬¦ï¼Œç»“æŸå­—ç¬¦ï¼Œè¡¥å…¨å­—ç¬¦ä»¥åŠè¯å…¸ç­‰ç­‰
> - å®šä¹‰Datasetï¼šç”¨äºå¾—åˆ°æ•°æ®é›†ï¼Œç»§æ‰¿è‡ªpytorchçš„Datasetã€‚æ­¤æ—¶æ•°æ®é›†é‡Œæ¯ä¸€ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ª ç»è¿‡ Fieldå£°æ˜çš„é¢„å¤„ç† é¢„å¤„ç†åçš„ wordlist
> - å»ºç«‹vocabï¼šåœ¨è¿™ä¸€æ­¥å»ºç«‹è¯æ±‡è¡¨ï¼Œè¯å‘é‡(word embeddings)
> - æ„é€ è¿­ä»£å™¨Iteratorï¼š: ä¸»è¦æ˜¯æ•°æ®è¾“å‡ºçš„æ¨¡å‹çš„è¿­ä»£å™¨ã€‚æ„é€ è¿­ä»£å™¨ï¼Œæ”¯æŒbatchå®šåˆ¶ç”¨æ¥åˆ†æ‰¹æ¬¡è®­ç»ƒæ¨¡å‹ã€‚

```python
# For data loading.
from torchtext import data, datasets

if True:
    import spacy
    spacy_de = spacy.load('de') #åŠ è½½å¾·è¯­è¯­è¨€æ¨¡å‹
    spacy_en = spacy.load('en') #åŠ è½½è‹±è¯­è¯­è¨€æ¨¡å‹
	
    
    """
   åœ¨æ–‡æœ¬å¤„ç†çš„è¿‡ç¨‹ä¸­ï¼ŒspaCyé¦–å…ˆå¯¹æ–‡æœ¬åˆ†è¯ï¼ŒåŸå§‹æ–‡æœ¬åœ¨ç©ºæ ¼å¤„åˆ†å‰²ï¼Œç±»ä¼¼äºtext.split(' ')ï¼Œç„¶ååˆ†è¯å™¨ï¼ˆTokenizerï¼‰ä»å·¦å‘å³ä¾æ¬¡å¤„ç†token
    """
    def tokenize_de(text): #Tokenizer:åˆ†è¯å™¨  è¿›è¡Œå¾·è¯­åˆ†è¯  
        #textï¼šè¾“å…¥çš„æ®µè½å¥å­  tok.textï¼šåˆ†åçš„tokenè¯
        return [tok.text for tok in spacy_de.tokenizer(text)]

    def tokenize_en(text): # è¿›è¡Œè‹±è¯­åˆ†è¯
        return [tok.text for tok in spacy_en.tokenizer(text)]

    BOS_WORD = '<s>'  #å¼€å§‹ç¬¦
    EOS_WORD = '</s>' #ç»ˆæ­¢ç¬¦
    BLANK_WORD = "<blank>" #ç©ºæ ¼
    
    # æ„å»ºFiledå¯¹è±¡ï¼Œå£°æ˜å¦‚ä½•å¤„ç†æ•°æ®ã€‚ä¸»è¦åŒ…å«ä»¥ä¸‹æ•°æ®é¢„å¤„ç†çš„é…ç½®ä¿¡æ¯ï¼Œæ¯”å¦‚æŒ‡å®šåˆ†è¯æ–¹æ³•ï¼Œæ˜¯å¦è½¬æˆå°å†™ï¼Œ		#èµ·å§‹å­—ç¬¦ï¼Œç»“æŸå­—ç¬¦ï¼Œè¡¥å…¨å­—ç¬¦ä»¥åŠè¯å…¸ç­‰ç­‰
    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD) #å¾—åˆ°æºå¥å­
    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD,  
                     eos_token = EOS_WORD, pad_token=BLANK_WORD)

    MAX_LEN = 100 #æœ€å¤§é•¿åº¦
    
    # https://s3.amazonaws.com/opennmt-models/iwslt.pt æ•°æ®é›†
    #åŒæ—¶å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿˜æœ‰æµ‹è¯•é›†çš„æ„å»ºï¼Œæ­¤æ—¶æ•°æ®é›†é‡Œæ¯ä¸€ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ª ç»è¿‡ Fieldå£°æ˜çš„é¢„å¤„ç† é¢„å¤„ç†åçš„ 	#wordlist
    train, val, test = datasets.IWSLT.splits(
        exts=('.de', '.en')   # æ„å»ºæ•°æ®é›†æ‰€éœ€çš„æ•°æ®é›†
        , fields=(SRC, TGT),  #å¦‚ä½•èµ‹å€¼ç»™trainé‚£ä¸‰ä¸ªçš„ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and 
            len(vars(x)['trg']) <= MAX_LEN)  #æºå¥å­å’Œç›®æ ‡å¥å­é•¿åº¦å°äº100çš„ç­›é€‰å‡ºæ¥
    
    MIN_FREQ = 2 #å®šä¹‰æœ€å°é¢‘ç‡
    
    #å»ºç«‹è¯æ±‡è¡¨ï¼Œè¯å‘é‡(word embeddings)ã€‚å³éœ€è¦ç»™æ¯ä¸ªå•è¯ç¼–ç ï¼Œç„¶åè¾“å…¥æ¨¡å‹
    #bulid_vocab()æ–¹æ³•ä¸­ä¼ å…¥ç”¨äºæ„å»ºè¯è¡¨çš„æ•°æ®é›†
    SRC.build_vocab(train.src, min_freq=MIN_FREQ) 
    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)
    
    #ä¸€æ—¦è¿è¡Œäº†è¿™äº›ä»£ç è¡Œï¼ŒSRC.vocab.stoiå°†æ˜¯ä¸€ä¸ªè¯å…¸ï¼Œå…¶è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ä½œä¸ºé”®ï¼Œè€Œå…¶å¯¹åº”çš„ç´¢å¼•ä½œä¸ºå€¼ï¼› 	#SRC.vocab.itoså°†æ˜¯ç›¸åŒçš„å­—å…¸ï¼Œå…¶ä¸­çš„é”®å’Œå€¼è¢«äº¤æ¢ã€‚
```



> æ‰¹è®­ç»ƒå¯¹äºé€Ÿåº¦æ¥è¯´å¾ˆé‡è¦ã€‚å¸Œæœ›æ‰¹æ¬¡åˆ†å‰²éå¸¸å‡åŒ€å¹¶ä¸”å¡«å……æœ€å°‘ã€‚ è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬**å¿…é¡»ä¿®æ”¹torchtexté»˜è®¤çš„æ‰¹å¤„ç†å‡½æ•°**ã€‚ è¿™éƒ¨åˆ†ä»£ç ä¿®è¡¥å…¶é»˜è®¤æ‰¹å¤„ç†å‡½æ•°ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬æœç´¢è¶³å¤Ÿå¤šçš„å¥å­ä»¥æ„å»ºç´§å¯†æ‰¹å¤„ç†ã€‚  ä¸€èˆ¬æ¥è¯´ç›´æ¥è°ƒç”¨`BucketIterator` ï¼ˆè®­ç»ƒç”¨ï¼‰å’Œ `Iterator`ï¼ˆæµ‹è¯•ç”¨ï¼‰ å³å¯
>
> `BucketIterator`å’Œ`Iterator`çš„åŒºåˆ«æ˜¯ï¼ŒBucketIteratorå°½å¯èƒ½çš„æŠŠé•¿åº¦ç›¸ä¼¼çš„å¥å­æ”¾åœ¨ä¸€ä¸ªbatché‡Œé¢ã€‚

#### Iterators

```python
"""
å®šä¹‰ä¸€ä¸ªè¿­ä»£å™¨ï¼Œè¯¥è¿­ä»£å™¨å°†ç›¸ä¼¼é•¿åº¦çš„ç¤ºä¾‹æ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚ åœ¨ä¸ºæ¯ä¸ªæ–°çºªå…ƒ(epoch)ç”Ÿäº§æ–°é²œæ”¹ç»„çš„æ‰¹æ¬¡æ—¶ï¼Œæœ€å¤§ç¨‹åº¦åœ°å‡å°‘æ‰€éœ€çš„å¡«å……é‡ã€‚
"""
class MyIterator(data.Iterator):
    def create_batches(self):
        #åœ¨trainçš„æ—¶å€™ï¼Œè¦è¿›è¡Œsortï¼Œå°½é‡å‡å°‘padding
        #ç›®çš„æ˜¯è‡ªåŠ¨è¿›è¡Œshuffleå’Œpaddingï¼Œå¹¶ä¸”ä¸ºäº†è®­ç»ƒæ•ˆç‡æœŸé—´ï¼Œå°½é‡æŠŠå¥å­é•¿åº¦ç›¸ä¼¼çš„shuffleåœ¨ä¸€èµ·ã€‚
        if self.train:
            def pool(d, random_shuffler):
                for p in data.batch(d, self.batch_size * 100):
                    p_batch = data.batch(
                        sorted(p, key=self.sort_key), #æŒ‰ç…§è¯çš„æ•°å¤§å°æ’åº
                        self.batch_size, self.batch_size_fn)
                    for b in random_shuffler(list(p_batch)):
                        yield b #bå°±æ˜¯batchï¼Œ ç±»æ¯”ä¸Šè¿°çš„gen_dataå‡½æ•°
            self.batches = pool(self.data(), self.random_shuffler) #è°ƒç”¨pool
            
         #åœ¨valid+test(éªŒè¯é›†å’Œæµ‹è¯•é›†)çš„æ—¶å€™  å’Œä¸Šé¢å…·ä½“åŒºåˆ«åœ¨å“ªï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
        else:
            self.batches = []
            for b in data.batch(self.data(), self.batch_size,
                                          self.batch_size_fn):
                self.batches.append(sorted(b, key=self.sort_key))

def rebatch(pad_idx, batch):  #pad_idxï¼šç©ºæ ¼é”®
    "Fix order in torchtext to match ours"
    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)#ä¸ºä»€ä¹ˆè¦è¿›è¡Œ
    return Batch(src, trg, pad_idx) #è°ƒç”¨ä¸Šè¿°çš„Batchç±»   pad_idxå°±æ˜¯pad
```

#### Multi-GPU Training



> æœ€åä¸ºäº†çœŸæ­£åœ°å¿«é€Ÿè®­ç»ƒï¼Œå°†ä½¿ç”¨å¤šä¸ªGPUã€‚ è¿™éƒ¨åˆ†ä»£ç å®ç°äº†å¤šGPUå­—ç”Ÿæˆï¼Œå®ƒä¸æ˜¯Transformerç‰¹æœ‰çš„ã€‚ å…¶**æ€æƒ³æ˜¯å°†è®­ç»ƒæ—¶çš„å•è¯ç”Ÿæˆåˆ†æˆå—ï¼Œä»¥ä¾¿åœ¨è®¸å¤šä¸åŒçš„GPUä¸Šå¹¶è¡Œå¤„ç†ã€‚** æˆ‘ä»¬ä½¿ç”¨PyTorchå¹¶è¡ŒåŸè¯­æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼š
>
> 
>
> - replicate -å¤åˆ¶ - å°†æ¨¡å—æ‹†åˆ†åˆ°ä¸åŒçš„GPUä¸Š
> - scatter -åˆ†æ•£ - å°†æ‰¹æ¬¡æ‹†åˆ†åˆ°ä¸åŒçš„GPUä¸Š
> - parallel_apply -å¹¶è¡Œåº”ç”¨ - åœ¨ä¸åŒGPUä¸Šå°†æ¨¡å—åº”ç”¨äºæ‰¹å¤„ç†
> - gather - èšé›† - å°†åˆ†æ•£çš„æ•°æ®èšé›†åˆ°ä¸€ä¸ªGPUä¸Š
> - nn.DataParallel - ä¸€ä¸ªç‰¹æ®Šçš„æ¨¡å—åŒ…è£…å™¨ï¼Œåœ¨è¯„ä¼°ä¹‹å‰è°ƒç”¨å®ƒä»¬ã€‚

```python
# Skip if not interested in multigpu.
class MultiGPULossCompute:
    "A multi-gpu loss compute and train function."
    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):
        # Send out to different gpus.
        self.generator = generator
        self.criterion = nn.parallel.replicate(criterion, 
                                               devices=devices)
        self.opt = opt
        self.devices = devices
        self.chunk_size = chunk_size
        
    def __call__(self, out, targets, normalize):
        total = 0.0
        generator = nn.parallel.replicate(self.generator, 
                                                devices=self.devices)
        out_scatter = nn.parallel.scatter(out, 
                                          target_gpus=self.devices)
        out_grad = [[] for _ in out_scatter]
        targets = nn.parallel.scatter(targets, 
                                      target_gpus=self.devices)

        # Divide generating into chunks.
        chunk_size = self.chunk_size
        for i in range(0, out_scatter[0].size(1), chunk_size):
            # Predict distributions
            out_column = [[Variable(o[:, i:i+chunk_size].data, 
                                    requires_grad=self.opt is not None)] 
                           for o in out_scatter]
            gen = nn.parallel.parallel_apply(generator, out_column)

            # Compute loss. 
            y = [(g.contiguous().view(-1, g.size(-1)), 
                  t[:, i:i+chunk_size].contiguous().view(-1)) 
                 for g, t in zip(gen, targets)]
            loss = nn.parallel.parallel_apply(self.criterion, y)

            # Sum and normalize loss
            l = nn.parallel.gather(loss, 
                                   target_device=self.devices[0])
            l = l.sum()[0] / normalize
            total += l.data[0]

            # Backprop loss to output of transformer
            if self.opt is not None:
                l.backward()
                for j, l in enumerate(loss):
                    out_grad[j].append(out_column[j][0].grad.data.clone())

        # Backprop all loss through transformer.            
        if self.opt is not None:
            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]
            o1 = out
            o2 = nn.parallel.gather(out_grad, 
                                    target_device=self.devices[0])
            o1.backward(gradient=o2)
            self.opt.step()
            self.opt.optimizer.zero_grad()
        return total * normalize
```

> Now we create our model, criterion, optimizer, data iterators, and paralelization

```python
# GPUs to use
devices = [0, 1, 2, 3]
if True:
    pad_idx = TGT.vocab.stoi["<blank>"]
    model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)
    model.cuda()
    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)
    criterion.cuda()
    BATCH_SIZE = 12000
    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,
                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),
                            batch_size_fn=batch_size_fn, train=True)
    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,
                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),
                            batch_size_fn=batch_size_fn, train=False)
    model_par = nn.DataParallel(model, device_ids=devices)
None
```

> Now we **train the model**. I will play with the warmup steps a bit, but everything else uses the default parameters. On an AWS p3.8xlarge with 4 Tesla V100s, this runs at ~27,000 tokens per second with a batch size of 12,000
>
> åœ¨å…·æœ‰4ä¸ªTesla V100 GPUçš„AWS p3.8xlargeæœºå™¨ä¸Šï¼Œæ¯ç§’è¿è¡Œçº¦27,000ä¸ªè¯ï¼Œæ‰¹è®­ç»ƒå¤§å°ä¸º12,000ã€‚

#### Training the System

```python
#!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt
#è¿›è¡Œtrainå’Œeval
if False: # falseå­˜åœ¨çš„æ„ä¹‰åœ¨å“ªï¼Ÿï¼Ÿï¼Ÿ ä½¿ç”¨GPUï¼Ÿ
    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,
            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))
    for epoch in range(10):
        model_par.train()
        run_epoch((rebatch(pad_idx, b) for b in train_iter), 
                  model_par, 
                  MultiGPULossCompute(model.generator, criterion, 
                                      devices=devices, opt=model_opt))
        model_par.eval()
        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), 
                          model_par, 
                          MultiGPULossCompute(model.generator, criterion, 
                          devices=devices, opt=None))
        print(loss)
else:
    model = torch.load("iwslt.pt") #åŠ è½½æ‰€æœ‰çš„tensoråˆ°CPU	
```

> Once trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate.

```python
#ç±»æ¯”äºrun_epochå‡½æ•°  
for i, batch in enumerate(valid_iter):
    src = batch.src.transpose(0, 1)[:1]
    src_mask = (src != SRC.vocab.stoi["<blank>"]).unsqueeze(-2)
    out = greedy_decode(model, src, src_mask, 
                        max_len=60, start_symbol=TGT.vocab.stoi["<s>"])
    print("Translation:", end="\t")
    for i in range(1, out.size(1)):
        sym = TGT.vocab.itos[out[0, i]]
        if sym == "</s>": break
        print(sym, end =" ")
    print()
    print("Target:", end="\t")
    for i in range(1, batch.trg.size(0)):
        sym = TGT.vocab.itos[batch.trg.data[i, 0]]
        if sym == "</s>": break
        print(sym, end =" ")
    print()
    break
Translation:	<unk> <unk> . In my language , that means , thank you very much . 
Gold:	<unk> <unk> . It means in my language , thank you very much . 
```

